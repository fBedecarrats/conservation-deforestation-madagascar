[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Impact des aires protégées sur la déforestation : guide de formation pratique",
    "section": "",
    "text": "Ce contenu a été développé afin de servir de support pédagogique pour l’atelier “évaluation des politiques” de la session 2022 des Universités en sciences sociales Tany Vao. Les universités Tany Vao visent à dispenser une formation à la recherche de haut niveau à l’attention de doctorants et jeunes chercheurs de Madagascar et d’Afrique de l’Ouest. Après deux jours de plénières, les participants se répartissent pendant cinq jours entre quatre ateliers parallèles : socioéconomie, éthno-écologie, anthropologie et évaluation des politiques.\nL’atelier “évaluation des politiques” adopte une approche axée sur l’économétrie et la science des données. Il alterne des sessions théorique et pratique. Conformément au thème phare de Tany Vao pour 2022 (“environnement et sociétés”), le cas d’étude choisi pour servir de fil rouge à ces travaux est l’impact des aires protégées sur la déforestation.\n\nMise en garde : ce document est en cours de développement : il est encore incomplet et il est susceptible de contenir des erreurs. N’hésitez pas à les signaler en cliquant sur l’icône Github en haut à gauche et en créant un signalement (“Issue”).\n\nPhoto en couverture : “Déforestation à Madagascar” © IRD - Bernard Moizo"
  },
  {
    "objectID": "00-intro.html",
    "href": "00-intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Il s’agit d’un support de formation conçu pour initier des étudiants et jeunes chercheurs non familiers de l’économétrie spatiale, avec un objet très concret. Ce document a été élaboré avec les priorités suivantes :\n\nFamiliarisation avec la démarche scientifique : recherche bibliographique, revue d’articles économétriques ;\nFormulaion d’une question de recherche relevant de l’évaluation économétrique et recherche des donnéees permettant d’y répondre ;\nCompréhension des différentes approches pour constituer un contrefactuel : randomisation, appariement, comparaison avant-après, différence de différences ;\nFamiliarisation avec l’outil de traitement statistique R.\n\nD’autres aspects ont volontairement été placés au second plan, en particulier la discussion technique de la fiabilité, différences et biais des sources de données mobilisées, ou encore les subtilités associées aux modèles économétriques. Les méthodes économétriques mobilisées ici sont volontairement simples : pour être publiable dans une revue scientifique, un travail d’évaluation doit mobiliser des spécifications plus sophistiquées.\n\n\n\nRemplacer la variable de temps de parcours à une ville d’au moins 5000 habitants par une variable moins exposée à l’endogénéité, soit :\n\nLa distance à une ville d’au moins 5000 habitants en 2000 ; ou\nLa distance euclidienne à une ville, sans tenir compte du réseau de transport.\n\nAjouter de la discontinuité de la régression"
  },
  {
    "objectID": "00-intro.html#outils-utilisés",
    "href": "00-intro.html#outils-utilisés",
    "title": "Introduction",
    "section": "Outils utilisés",
    "text": "Outils utilisés\n\nNotebook Quarto\nLes éléments ci-dessous constituent le support pour les sessions pratiques de cet atelier. Ils sont réalisés en suivant une approche ouverte et reproductible fondée sur un document de type “notebook” (Bédécarrats and Hobeika 2017). Un notebook rassemble à la fois :\n\nles lignes de code du programme statistique qui traite les données ;\nles résultats (calculs, tableaux, graphiques…) produits lors de l’exécution de ce programme ;\nle texte rédigé par les auteurs pour expliquer le processus d’analyse et en interpréter les résultats.\n\nL’intérêt du format notebook, par rapport à l’utilisation de documents distincts pour traiter les données d’une part, et en analyser les résultats d’autre part, est multiple :\n\nfavoriser la reproductibité de la recherche (tout le processus de traitement, analyse, interprétation peut être inspecté et dupliqué) ;\nfaciliter le travail du chercheur (une interface pour tout faire) ; et\nassurer les meilleures pratiques de collaboration (utilisation pour le versionnage, partage et fusion des travaux les outils performants développés en programmation informatique).\n\nLes traitements sont réalisés en R, qui est à la fois un logiciel et un langage open sources dédiés à l’analyse de données. Les traitements R sont inclus dans un document Quarto, un format qui exécute aussi bien des codes en R, Python, e rendus dans différents formats (LaTeX/PDF, HTML ou Word).\nLa mise en forme des rendus Quarto est paramétrable. Ici, on a notamment placé un argument code-fold: true dans le fichier _quarto.yml. Cela fait que les blocs de code ne sont pas visible dans le rendu web par défaut : il faut cliquer sur “code” pour les déplier.\n\n\nMapme.biodiversity\nOn s’appuie sur le package R {mapme.biodiversity}, développé par la KfW dans le cadre de l’initiative commune MAPME qui associe la KfW et l’AFD. Le package {mapme.biodiversity} facilite l’acquisition et la préparation d’un grand nombre de données (CHIRPS, Global Forest Watch, FIRMS, SRTM, Worldpop…) et calculer un grand nombre d’indicateurs de manière harmonisée (active_fire_counts, biome classification, land cover classification, population count, precipitation, soil properties, tree cover loss, travel time…). Une documentation riche est disponible sur le portail dédié du package en question (Kluve et al. 2022).\nOn mobilise aussi les codes d’analyse d’impact développés par la même équipe et mises à disposition dans le dépôt Github: https://github.com/openkfw/mapme.protectedareas. Le code développé par l’équipe est assez complexe. A des fins pédagogiques et pour s’assurer qu’on l’a bien compris, on propose ici une version simplifiée (en cours de développement).\n\n\nOnyxia/SSP Cloud\nLes sources pour l’ensemble du code source et du texte du présent document est accessible sur Github à l’adresse suivante : https://github.com/fBedecarrats/conservation-deforestation-madagascar. Les analyses sont menées sur la plateforme SSP Cloud, mises à disposition par l’INSEE pour les data scientists travaillant pour des administrations publiques. Il s’agit d’une instance de stockage de données massif (S3) et de calcul haute performance (cluster Kubernetes) disposant d’une interface simplifiée permettant à l’utilisateur de configurer, lancer et administrer facilement des environnements de traitement de données (RStudio server, Jupyter lab ou autres…). Le code est conçu pour s’exécuter de la même manière en local sur un PC, mais la préparation des données sera certainement beaucoup plus longue à exécuter.\n\n\nLibrairies R\nOutre Mapme.biodiversity, on mobilise une série de librairies (appelées “packages” en R), qui facilitent grandement l’analyse. Elles sont listées dans le bloc ci-dessous.\n\n\nCode\n# # Le package est en cours de développement, toujours installer la version en cours\n# remotes::install_github(\"mapme-initiative/mapme.biodiversity\", \n#                         upgrade = \"always\")\n\nlibrairies_requises <- c( # On liste les librairies dont on a besoin\n  \"tidyverse\", # Une série de packages pour faciliter la manipulation de données\n  \"readxl\", # Pour lire les fichiers excel (Carvalho et al. 2018)\n  \"cowplot\", # Pour arranger des graphiques en illustrations composées\n  \"gt\", # Pour des rendus graphiques harmonisés html et pdf/LaTeX\n  \"sf\", # Pour faciliter la manipulation de données géographiques\n  \"wdpar\", # Pour télécharger simplement la base d'aires protégées WDPA\n  \"webdriver\", # requis pour installer phantomjs pour wdpar\n  \"tmap\", # Pour produire de jolies carte\n  \"geodata\", # Pour télécharger simplement les frontières administratives\n  \"tidygeocoder\", # pour obtenir les coordo GPS d'un point à partir de son nom\n  \"maptiles\", # Pour télécharger des fonds de carte \n  \"mapme.biodiversity\", # Acquisition et traitement des données du projet\n  \"plm\", # Linear Models for Panel Data and robust covariance matrices\n  \"broom\", # pour reformater simplement les rendus de tests statistiques\n  \"stargazer\", # Reformater de manière plus lisible les résumé des régressions\n  \"MatchIt\", # Pour le matching\n  #\"glm\", # Modèles linéaires généralisés (pour le PSM)\n  \"optmatch\", # Fonctions d'optimisation du matching\n  \"rgee\",\n  \"rgeeExtra\",\n  \"cobalt\") # Tables et graphs d'équilibre des groupes de matching\n  \n# On regarde parmi ces librairies lesquelles ne sont pas installées\nmanquantes <- !(librairies_requises %in% installed.packages())\n# On installe celles qui manquent\nif(any(manquantes)) install.packages(librairies_requises[manquantes])\n\n## On charge toutes les librairies requises\n## On fera le chargement dans le chapitres pour expliciter les manips\n# invisible(lapply(librairies_requises, require, character.only= TRUE))\n\n# TODO : repasser les paramètres ci-dessous en clair dans les chapitres\n# Système de coordonnées géographiques utilisées pour le projet : EPSG:29739\nmon_scr <- \"EPSG:29739\" # correspondant à Tananarive / UTM zone 39S\n# Surface des hexagones en km2\ntaille_hex <- 5\n# Taille des titres des cartes\ntaille_titres_cartes = 1\n# on crée un dossier de données si pas déjà disponible\ndir.create(\"data\")\n# Désactiver les notations scientifiques\noptions(scipen =999)"
  },
  {
    "objectID": "00-intro.html#mode-demploi",
    "href": "00-intro.html#mode-demploi",
    "title": "Introduction",
    "section": "Mode d’emploi",
    "text": "Mode d’emploi\n\n\n\n\nBédécarrats, Florent, and Alexandre Hobeika. 2017. “Une Alternative à Word : Écrire En RMarkdown.” Billet. Data Sciences Sociales. http://data.hypotheses.org/1144.\n\n\nKluve, Jochen, Johannes Schielain, Melvin Wong, and Yota Eilers. 2022. “The KfW Protected Areas Portfolio: A Rigorous Impact Evaluation.” Frankfürt."
  },
  {
    "objectID": "01-aires_protegees.html",
    "href": "01-aires_protegees.html",
    "title": "1  Aires protégées",
    "section": "",
    "text": "Les études sur les aires protégées s’appuient fréquemment sur la base WDPA (World Database on Protected Area), consultable en ligne sur https://protectedplanet.net. On s’aperçoit dans le cas de Madagascar que cette base de données comporte de nombreuses erreurs (qu’on étudiera plus bas). La base rassemblée par l’association Vahatra dans le cadre de la monographie qu’elle a coordonnée sur l’ensemble des aires protégées terrestres malgaches semble beaucoup plus fiable (Goodman et al. 2018). Les données en question sont disponibles sur le portail https://protectedareas.mg avec une licence creative commons (CC-BY).\nLe bloc de code ci-dessous (cliquer sur “code” pour visualiser), présente la séquence d’opérations réalisées pour préparer les données.Pour comprendre certaines opérations contenues dans le bloc de code, il est utile d’être familier de la syntaxe de R et des packages du tidyverse. Voir le chapitre Chapter 11.\n\n\nCode\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(sf)\nlibrary(tmap)\nlibrary(geodata)\nlibrary(cowplot)\nlibrary(wdpar)\nlibrary(gt) # Pour faciliter le rendu des tableaux (et ils sont jolis)\n\n# Le shapefile est composé d'une série de fichiers, (.shp, .dbf, .prj, .shx)\n# qui doivent avoir le même nom et être au même endroits pour être ouverts en\n# même temps. Comme souvent, ils sont compressés ensemble dans un fichier zip.\n# On commence par dézipper (décompresser) ce fichier.\nunzip(\"data/Vahatra98AP.zip\", exdir = \"data/Vahatra\")\n# On importe dans R en pointant vers le fichier .shp, mais c'est bien toute la\n# collection de fichiers homonymes .shp, .dbf, .shx qui est chargée.\nAP_Vahatra <- st_read(\"data/Vahatra/Vahatra98AP.shp\", quiet = TRUE) %>%\n  # Il manque la projection (pas de fichier .prj), on la spécifie à la main\n  st_set_crs(\"EPSG:4326\") # EPSG 4325 = WSG 84 = le standard pour le web\n\n# L'option ci-dessous est un peu cryptique : des caractéristiques topologiques\n# de la carte source sont incompatibles avec la possibilité d'avoir des objets\n# sphériques dans sf. Cela disparait si on désactive cette possibilité\nsf_use_s2(FALSE) \n\n# Identification des dates ----------------------------------------------------\n# Cette section est un brin complexe, à base de manipulation de chaînes de \n# caractères et de dates\n\n# Détecte les dates écrites 2 avril 2020 ou 02 avril 2020, etc.\ndate_ecrite <- \"[:digit:]{1,2} [:alpha:]* [:digit:]{4}\"\n# Détecte les dates écrites 02/04/20 ou 02.04.20 ou 02.04.2020, etc.\ndate_abrev <- \"[:digit:]{2}[:punct:][:digit:]{2}[:punct:][:digit:]{2,4}\"\n# Des années écrites à 2 chiffres\ndate_ecrite_an_abrev <- \"[:digit:]{1,2} [:alpha:]* [:digit:]{4}\"\n# Détecte l'une ou l'autre des formes précédentes\ntoute_date <- paste(date_ecrite, date_abrev, date_ecrite_an_abrev, sep = \"|\")\n# Détecte une mention d'année seule : 1984, 2015, etc.\nannee_seule <- \"[:digit:]{4}\"\n# Détecte les formes indicatrices d'un changement\nmention_changement <- \"Changement|changement|anciennement|actuel|auparavant\"\n# Une fonction qui traduit les dates écrites en toutes lettre du français à \n# l'anglais (pour les parser ensuite car ça ne fonctionne qu'en anglais)\ntrad_dates <- function(date_fr) {\n  str_replace_all(date_fr,\n                  c(\"janvier\" = \"January\",\n                    \"fevrier\" = \"February\",\n                    \"mars\" = \"March\",\n                    \"avril\" = \"April\",\n                    \"mai\" = \"May\",\n                    \"juin\" = \"June\",\n                    \"juillet\" = \"July\",\n                    \"aout\" = \"August\",\n                    \"septembre|setembre\" = \"September\",\n                    \"octobre\" = \"October\",\n                    \"novembre\" = \"November\",\n                    \"decembre|decmbre\" = \"December\"))\n}\n# Cette fonction remplace 01.04.58 par 01.04.1958 et marche avec . ou /\n# On indique avec limite le nombre d'année où on considère que c'est 1900 vs 2000\ncomplete_annee <- function(date_abrev, limite = 20) {\n  if (str_detect(date_abrev, \"([:punct:])([:digit:]{2})[:punct:]?$\")) {\n    date_abrev <- str_remove(date_abrev, \":punct:]?$\")\n    if (as.numeric(str_extract(date_abrev, \"[:digit:]{2}$\")) > limite) {\n      date_abrev <- str_replace(date_abrev, \n                                \"([:punct:])([:digit:]{2})[:punct:]?$\", \"\\\\119\\\\2\")\n    } else {\n      date_abrev <- str_replace(date_abrev, \n                                \"([:punct:])([:digit:]{2})[:punct:]?$\", \"\\\\120\\\\2\")\n    }\n  }\n  return(date_abrev)\n}\n# La fonction précédente est unitaire, on la transforme pour qu'elle s'applique à une liste.\ncomplete_liste_dates <- function(liste_dates) {\n  map(liste_dates, complete_annee)\n}\n\nAP_Vahatra <- AP_Vahatra %>%\n  # On extrait les dates des champs de texte\n  mutate(date_creation = str_extract_all(creation, toute_date), \n         # Une date a un format incohérent, on la recode à la main\n         date_creation = ifelse(creation == \"Créée le 07 aout 04\",\n                                \"07 aout 2004\", date_creation),\n         date_creationA = map(date_creation, 1), # La 1ère date\n         date_creationB = map(date_creation, 2)) %>% # Si 2 dates, la seconde\n  # On traduit les mois en anglais pour une conversion au format date\n  mutate(across(c(date_creationA, date_creationB), trad_dates)) %>%\n  mutate(across(c(date_creationA, date_creationB), complete_liste_dates)) %>%\n  mutate(across(c(date_creationA, date_creationB), dmy)) %>%\n  mutate(date_creation = case_when(is.na(date_creationB) ~ date_creationA,\n                                    date_creationA > date_creationB ~ date_creationB,\n                                    date_creationA <= date_creationB ~ date_creationA),\n         date_modification = case_when(is.na(date_creationB) ~ date_creationB,\n                                       date_creationA < date_creationB ~ date_creationB,\n                                       date_creationA >= date_creationB ~ date_creationA),\n         # On repère si il y a eu un changement de statut ou de frontières\n         mention_changement = str_detect(creation, mention_changement)) %>%\n    # On enlève les colonnes inutiles\n  select(-date_creationA, -date_creationB) %>%\n  # On place les colonnes créées à gauche pour les inspecter facilement\n  relocate(date_creation:mention_changement, .after = creation) \n\n# Après une vérification manuelle, on remarque les données de l'association Vahatra comportent des mentions incomplètes pour certaines aires, qui n'ont pas été extraites:\n\n\n# Lokobe : 31 décembre 1927\nAP_Vahatra <- AP_Vahatra %>%\n  mutate(date_creation = case_when(nom == \"Lokobe\" ~ ymd(\"1927-12-31\"),\n                                   nom == \"Mantadia\" ~ ymd(\"1989-01-11\"),\n                                   TRUE ~ date_creation),\n         date_modification = case_when(nom == \"Bemaraha\" ~ ymd(\"2011-07-06\"),\n                                       nom == \"Lokobe\" ~ ymd(\"2011-07-06\"),\n                                       nom == \"Tsaratanana\" ~ ymd(\"2011-07-06\"),\n                                       nom == \"Pic d'Ivohibe\" ~ ymd(\"2015-04-28\"),\n                                       nom == \"Mantadia\" ~ ymd(\"2002-08-07\"),\n                                       TRUE ~ date_modification)) %>%\n  st_make_valid() # fiabilise qu'il n'y a pas d'erreurs topologiques\n# dir.create(\"AP_Vahatra\")\n# st_write(AP_Vahatra, \"out/AP_Vahatra.shp\")\n# writexl::write_xlsx(st_drop_geometry(AP_Vahatra), \"AP_Vahatra.xlsx\")\n\n\nLe bloc de code suivant génère une carte interactive. On a également inclus des lignes de code qui permettent de formater la carte joliment pour un rendu figé (pdf/LaTeX, html statique, word), mais ce code est “commenté”, c’est-à-dire qu’on a placé des dièses au début de chaque ligne, de sorte qu’il ne s’exécute pas (R n’exécute jamais ce qui se trouve à droite d’un # sur une ligne). Pour plus de détails sur la manière dont on produit des cartes, voire l’annexe : Cartes simples en R\n\n\nCode\nif (file.exists(\"data/contour_mada.rds\")) {\n  load(\"data/contour_mada.rds\")\n} else {\n  contour_mada <- gadm(country = \"Madagascar\", resolution = 1, level = 0,\n                     path = \"data/GADM\") %>%\n  st_as_sf()\n# On enregistre contour_mada pour s'en servir par la suite\nsave(contour_mada, file = \"data/contour_mada.rds\")\n}\n\n# On génère un rendu cartographique\ntmap_mode(\"view\") # En mode interactif\ntm_shape(contour_mada) +\n  tm_borders() +\n  tm_shape(AP_Vahatra) + \n  tm_polygons(col = \"cat__iucn\", alpha = 0.6, title = \"Catégorie IUCN\") +\n  tmap_options(check.and.fix = TRUE) # +\n\n\n\n\n\n\n\nCode\n# Les dièses en début de ligne font que ce qui suit ne s'exécute pas.\n# La suite est uniquement pour les rendus fixes (tmap_mode = \"plot\"), p. ex. pour les pdf\n  # # NB : on note les positions en majuscules quand on veut coller aux marges\n  # tm_credits(\"Sources: WDPA et GADM\", position = c(\"RIGHT\", \"BOTTOM\"),\n  #            size = 0.6) +\n  # tm_layout(main.title = \"Aires protégées de Madagascar\",\n  #           # NB : position en minuscules pour laisser un espace avec la marge\n  #           main.title.position = c(\"center\", \"top\"),\n  #           main.title.size = taille_titres_cartes,\n  #           legend.position = c(\"left\", \"top\"),\n  #           legend.outside = TRUE)\n\n\nOn peut également réaliser un graphique qui présente l’historique de création des aires protégées. Pour plus de précisions sur la manière de produire des graphiques en R, voir l’annexe correspondante.\n\n\nCode\n# On ordonne les nom d'aires protégées dans l'ordre de leur séquence de création\nordre_chrono_AP <- AP_Vahatra %>%\n  arrange(desc(date_creation), desc(nom)) %>%\n  pull(nom)\n# On transforme le champ \"nom\" de caractère, à une catégorisation ordonnée où\n# l'ordre correspond \nAP_Vahatra_carte <- AP_Vahatra %>%\n  mutate(nom = factor(nom, levels = ordre_chrono_AP),\n         cat_taille = case_when(hectares > 300000 ~ 2,\n                                hectares > 150000 ~ 1.5,\n                                hectares >  50000 ~ 1,\n                                             TRUE ~ 0.5)) %>%\n  rename(`Catégorie IUCN` = cat__iucn)\n\n# On crée un graph pour les anciennetés\ngraph_gauche <- AP_Vahatra_carte %>%\n  ggplot(aes(x = date_creation, xend = ymd(\"2022-10-01\"), y = nom, yend = nom, \n                   color = `Catégorie IUCN`)) +\n  geom_segment(size = 2) +\n  ggtitle(\"Ancienneté\") +\n  theme(axis.ticks.y = element_blank(),\n        legend.position = \"none\",\n        axis.title = element_blank(),\n        axis.text.x.bottom =  element_text(angle = 45, hjust = 1),\n        axis.text.x.top = element_text(angle = -45, hjust = 1)) + \n  scale_x_date(sec.axis = dup_axis())\n\ngraph_droite <- AP_Vahatra_carte %>%\n  ggplot(aes(x = 0, xend = hectares/100, y = nom, yend = nom, \n                   color = `Catégorie IUCN`)) +\n  geom_segment(size = 2) + \n  ggtitle(\"Surface (km2)\") +\n  theme(axis.text.y = element_blank(),\n        axis.title = element_blank(),\n        axis.text.x.bottom =  element_text(angle = 45, hjust = 1),\n        axis.text.x.top = element_text(angle = -45, hjust = 0),\n        legend.position = \"none\") + \n  scale_x_continuous(sec.axis = dup_axis())\n\nlegende <- get_legend(graph_gauche  +\n                        guides(color = guide_legend(nrow = 1)) +\n                        theme(legend.position = \"bottom\"))\n\n# On colle les deux\ngraphs <- plot_grid(graph_gauche, graph_droite, rel_widths = c(2.2, 1),\n          nrow = 1)\nplot_grid(graphs, legende, ncol = 1,\n          rel_heights = c(1,.1))\n\n\n\n\n\nIl faut aussi s’assurer qu’on filtre bien les entités analysées selon un critère pertinent. Actuellement, on exclut les aires marines. Il pourrait toutefois sembler utile d’écarter les aires dont le statut de protection est considéré comme trop faible. Il pourrait aussi être pertinent de ne garder que les aires protégées comportant un niveau minimum de couvert forestier : autrement, cela signifie que la forêt n’est pas un habitat pertinent pour les écosystèmes que la démarche de conservation cherche à protéger dans cette aire."
  },
  {
    "objectID": "01-aires_protegees.html#world-database-on-protected-areas",
    "href": "01-aires_protegees.html#world-database-on-protected-areas",
    "title": "1  Aires protégées",
    "section": "1.2 World Database on Protected Areas",
    "text": "1.2 World Database on Protected Areas\nOn commence par télécharger et présenter ces données.\n\n\nCode\n# On regarde si les données WDPA sont disponibles sur l'ordinateur qui exécute\nif (file.exists(\"data/WDPA/WDPA_Oct2022_MDG-shapefile.zip\")) {\n  # Si oui, on charge\n  WDPA_Mada <- wdpa_read(\"data/WDPA/WDPA_Oct2022_MDG-shapefile.zip\")\n} else {\n  # Si non, on télécharge depuis protectedplanet\n  WDPA_Mada <- wdpa_fetch(\"Madagascar\", wait = TRUE,\n                      download_dir = \"data/WDPA\") \n}\n\n\n\n# TODO: inclure graph et description des données."
  },
  {
    "objectID": "01-aires_protegees.html#comparaison-des-données-vahatra-et-wdpa",
    "href": "01-aires_protegees.html#comparaison-des-données-vahatra-et-wdpa",
    "title": "1  Aires protégées",
    "section": "1.3 Comparaison des données Vahatra et WDPA",
    "text": "1.3 Comparaison des données Vahatra et WDPA\nOn commence par visualiser les différences spatiales entre les polygones, en affichant les 10 qui sont les plus différents entre les WDPA et Vahatra.\n\n\nCode\n# On harmonise les noms qui sont parfois notés différemment entre les sources\nAP_Vahatra <- AP_Vahatra %>%\n  mutate(nom_wdpa = case_when(\n    nom == \"Corridor Forestier Bongolava\" ~ \"Corridor forestier Bongolava\",\n    nom == \"Ranobe PK32\" ~ \"Ranobe PK 32\",\n    str_detect(nom, \"Ambositra-Vondrozo\") ~ \"Corridor Forestier Ambositra Vondrozo\",\n    nom == \"Réserve deTampolo\" ~ \"Réserve de Tampolo\",\n    nom == \"Bombetoka Beloboka\" ~ \"Bombetoka Belemboka\",\n    nom == \"Ampananganandehibe-Behasina\" ~ \"Ampanganandehibe-Behasina\",\n    nom == \"Forêt Sacrée Alandraza Analavelo\" ~ \"Analavelona\", # vérfié sur carte : les mêmes\n    nom == \"Réserve speciale Pointe à Larrée\" ~ \"Réserve spéciale Pointe à Larrée\", \n    nom == \"Vohidava-Betsimalaho\" ~ \"Vohidava Betsimalao\", \n    nom == \"Anjanaharibe Sud\" ~ \"Anjanaharibe_sud\",\n    nom == \"Iles Radama/Sahamalaza\" ~ \"Sahamalaza Iles Radama\",\n    nom == \"Kalambatritra\" ~ \"Kalambatrika\",\n    nom == \"Mananara-Nord\" ~ \"Mananara Nord\",\n    nom == \"Kirindy - Mitea\" ~ \"Kirindy Mite\",\n    nom == \"Midongy du Sud\" ~ \"Befotaka Midongy\", # Vérifié sur la carte\n    nom == \"Montagne d'Ambre/Forêt d'Ambre\" ~ \"Montagne d'Ambre\",\n    nom == \"Tsimanampesotsa\" ~ \"Tsimanampesotse\",\n    nom == \"Pic d'Ivohibe\" ~ \"Ivohibe\",\n    nom == \"Forêt Naturelle de Petriky\" ~ \"Forêt Naturel de Petriky\",\n    nom == \"Tsingy de Namoroka\" ~ \"Namoroka\",\n    nom == \"Réserve de Ressources Naturelle Mahimborondro\" ~ \"Mahimborondro\",\n    str_detect(nom, \"Complexe Tsimembo Manambolomaty\") ~ \"Complexe Tsimembo Manambolomaty\",\n    nom == \"Mandrozo\" ~ \"Zone Humide de Mandrozo\",\n    nom == \"Paysage Harmonieux Protégés Bemanevika\" ~ \"Complexe des Zones Humides de Bemanevika\",\n    nom == \"Nord Ifotaky\" ~ \"INord fotaky\",\n    TRUE ~ nom)) %>%\n  arrange(nom_wdpa) %>%\n  mutate(rownum = row_number())\n\n# On sauvegarde le résultat\nsave(AP_Vahatra, file = \"data/ch1_AP_Vahatra.rds\")\n\n# On ne garde que les aires de WDPA qui apparaissent dans Vahatra\nWDPA_commun <- WDPA_Mada %>%\n  filter(NAME %in% AP_Vahatra$nom_wdpa) %>%\n  filter(!(NAME == \"Analalava\" & IUCN_CAT == \"Not Reported\")) %>%\n  filter(!(NAME == \"Site Bioculturel d'Antrema\" & IUCN_CAT == \"Not Reported\")) %>%\n  filter(DESIG != \"UNESCO-MAB Biosphere Reserve\") %>%\n  arrange(NAME)  %>%\n  mutate(rownum = row_number())\n       \n# Cette fonction calcule la part d'un polygone incluse dans un \n# autre polygone et retourne un ratio entre 0 et 1\nratio_inclus <- function(x, y) {\n  inclus <- st_intersection(x, y)\n  ratio <- st_area(inclus) / st_area(x)\n  return(ratio)\n}\n\n# On calcule la part des polygones Vahatra incluse dans les polgones WDPA \nV_in_W <- map2_dbl(WDPA_commun$geometry, AP_Vahatra$geometry, ratio_inclus)\n# Puis l'inverse\nW_in_V <- map2_dbl(AP_Vahatra$geometry, WDPA_commun$geometry, ratio_inclus)\n# On fait un facteur des deux\nrecoupement_mutuel <- V_in_W * W_in_V\n# Qu'on ramène dans les jeux de données d'origine\nWDPA_commun2 <- bind_cols(WDPA_commun, V_in_W = V_in_W, W_in_V = W_in_V,\n                         recoupement_mutuel = recoupement_mutuel) %>%\n  arrange(recoupement_mutuel, rownum)\nAP_Vahatra2 <- bind_cols(AP_Vahatra, V_in_W = V_in_W, W_in_V = W_in_V,\n                        recoupement_mutuel = recoupement_mutuel) %>%\n  arrange(recoupement_mutuel, rownum)\n\n# On prend maintenant les 5 les plus éloignés et on les visualise\nmin_recoup <- WDPA_commun2 %>%\n  filter(row_number() <= 10) %>%\n  select(nom_wdpa = NAME, rownum) %>%\n  mutate(source = \"WDPA\") %>%\n  bind_rows(select(filter(AP_Vahatra2, rownum %in% .$rownum), nom_wdpa, rownum)) %>%\n  mutate(source = ifelse(is.na(source), \"Vahatra\", source))\ntmap_mode(\"plot\")\nmin_recoup %>%\n  tm_shape() +\n  tm_polygons() +\n  tm_facets(by = c(\"nom_wdpa\", \"source\")) +\n  tm_layout(panel.label.size=3)\n\n\n\n\n\nOn peut également comparer ceux pour lesquels on a des différences de date ou de statut.\n\n\nCode\n# On garde seulement les métadonnées qu'on veut comparer\nWDPA_a_comparer <- WDPA_commun %>% # On repart des AP communes\n  st_drop_geometry() %>% # Plus besoin de spatial\n  select(nom_wdpa = NAME, type_wdpa = INT_CRIT, cat_iucn_wdpa = IUCN_CAT,\n         year_wdpa = STATUS_YR) # On ne garde que les colonnes à comparer\n\nverif_meta_wdpa <-AP_Vahatra %>%\n  st_drop_geometry() %>% # Pas besoin d'un jeu spatial\n  select(nom:date_modification, nom_wdpa) %>% # colonnes à garder dans Vahatra\n  # On renomme la catégorie IUCN de Vahatra et on code les NA comme dans WDPA\n  mutate(cat_iucn = ifelse(is.na(cat__iucn), \"Not Reported\", cat__iucn)) %>%\n  relocate(cat_iucn, .before = cat__iucn) %>% # Nouvelle colonne près de l'ancienne\n  left_join(WDPA_a_comparer, by = \"nom_wdpa\") %>% # On rassemble Vahatra et WDPA\n  select(-nom_wdpa, -cat__iucn) %>% # On enlève les colonnes inutiles\n  # On compare les dates et statuts\n  mutate(`Différence de date` = year(date_creation) != year_wdpa,\n         `Différence de statut` = cat_iucn != cat_iucn_wdpa)\n\nverif_meta_wdpa %>%\n  summarise(`Nombre d'aires protégées comparées` = n(),\n            `Différence de date` = sum(`Différence de date`),\n            `Différence de statut` = sum(`Différence de statut`)) %>%\n  gt() %>%\n  tab_header(title = paste(\"Différences entre les données de WDPA et celles de\",\n                     \"l'assciation Vahatra sur les aires protégées terrestres\",\n                     \"à Madagascar\"))\n\n\n\n\n\n\n  \n    \n      Différences entre les données de WDPA et celles de l'assciation Vahatra sur les aires protégées terrestres à Madagascar\n    \n    \n  \n  \n    \n      Nombre d'aires protégées comparées\n      Différence de date\n      Différence de statut\n    \n  \n  \n    98\n68\n40\n  \n  \n  \n\n\n\n\nDans les cas qu’on peut comparer, les données de l’association Vahatra semblent plus fiables. On va donc privilégier l’utilisation de ces dernières.\nOn va également visualiser les aires de WDPA qui ne sont pas contenues dans Vahatra.\n\n\nCode\nWDPA_exclu <- WDPA_Mada %>%\n  filter(!(NAME %in% AP_Vahatra$nom_wdpa))\n\ntmap_mode(\"view\")\nWDPA_exclu %>%\n  tm_shape() +\n  tm_polygons(col = \"IUCN_CAT\")\n\n\n\n\n\n\n\nCode\nratio_terrestre <- function(x) {\n  inclus <- st_intersection(x, contour_mada$geometry)\n  ratio <- st_area(inclus) / st_area(x)\n  return(ratio)\n}\n\n# On crée un grand polygone avec toutes les AP dans Vahatra\nAP_Vahatra_fusion <- st_union(AP_Vahatra)\n\n# On calcule pour les aires protégées de WDPA qui ne sont pas dans Vahatra\n# dans quelle mesure elles sont terrestres et pas superposées à d'autres AP\n# déjà dans Vahatra\nWDPA_exclu <- WDPA_exclu %>%\n  filter(st_geometry_type(.) == \"MULTIPOLYGON\") %>%\n  mutate(part_terrestre = map2_dbl(.$geometry, contour_mada$geometry, \n                                   ratio_inclus),\n         part_deja_autre = map2_dbl(.$geometry, AP_Vahatra_fusion, \n                                    ratio_inclus))\n\n# On garde celles qui sont au moins 25% terrestre et 75% pas superposées\nWDPA_a_inclure <- WDPA_exclu %>%\n  filter(part_terrestre >= 0.25 & part_deja_autre <= 0.25) %>%\n  mutate(full_name = paste(INT_CRIT, NAME)) %>%\n  select(nom = NAME, WDPAID, full_name, creation = STATUS_YR)\n\n\nOn a visiblement des aires protégées qu’il serait pertinent d’inclure et qui ne sont pas dans Vahatra."
  },
  {
    "objectID": "01-aires_protegees.html#enjeux-de-fiabilité-des-données-daires-protégées",
    "href": "01-aires_protegees.html#enjeux-de-fiabilité-des-données-daires-protégées",
    "title": "1  Aires protégées",
    "section": "1.4 Enjeux de fiabilité des données d’aires protégées",
    "text": "1.4 Enjeux de fiabilité des données d’aires protégées\nImportant pour l’analyse : si périmètres pas juste => phénomènes de leakage, faux positifs ou faux négatifs.\nEnjeu aussi des métadonnées : date ou type sont importants pour l’analyse et celle-ci perd en fiabilité si ces informations ne sont pas correctes.\n\n\n\n\nGoodman, Steven M., Marie Jeanne Raherilalao, Sébastien Wohlhauser, Jean Clarck N. Rabenandrasana, Herivololona M. Rakotondratsimba, Fanja Andriamialisoa, and Malalarisoa Razafimpahanana. 2018. Les Aires Protégées Terrestres de Madagascar: Leur Histoire, Description Et Biote. Association Vahatra."
  },
  {
    "objectID": "02-caracteristiques_AP.html",
    "href": "02-caracteristiques_AP.html",
    "title": "2  Caractéristiques spatiales",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(mapme.biodiversity)\nlibrary(sf)\n\nif (file.exists(\"data/Vahatra_poly.rds\")) {\n  load(\"data/Vahatra_poly.rds\")\n} else {\n\n  Vahatra_poly <- AP_Vahatra %>%\n    filter(st_geometry_type(.) == \"MULTIPOLYGON\") %>%\n    st_cast(\"POLYGON\")\n  \n  # Constitution d'un portefeuille (voir la documentation)\n  Vahatra_poly <- init_portfolio(x = Vahatra_poly, \n                                 years = 2000:2020,\n                                 outdir = \"data/mapme_Vahatra\",\n                                 cores = 24,\n                                 add_resources = TRUE,\n                                 verbose = TRUE)\n  \n  # Données d'accessibilité de Nelson et al. (2018)\n  Vahatra_poly <-  get_resources(x = Vahatra_poly, resource = \"nelson_et_al\",  \n                                 range_traveltime = \"5k_110mio\")\n  # Modèle numérique de terrain SRTM de la NASA\n  Vahatra_poly <- get_resources(x = Vahatra_poly , resource = \"nasa_srtm\")\n  \n    # Indicateurs d'accessibilité\n  Vahatra_poly <- calc_indicators(x = Vahatra_poly,\n                                  \"traveltime\",  stats_accessibility = \"mean\",\n                                  engine = \"extract\")\n  # Indicateurs de relief de terrain\n  Vahatra_poly <- calc_indicators(x = Vahatra_poly,\n                                  indicators = c(\"tri\", \"elevation\"),\n                                  stats_tri = \"mean\", stats_elevation = \"mean\")\n  \n  #   # On récupère aussi les données de Global Forest Watch sur le couver forestier\n  # Vahatra_poly <- get_resources(x = Vahatra_poly, \n  #                               resources = c(\"gfw_treecover\", \"gfw_lossyear\"))\n  #   # Indicateurs de couvert forestier\n  # Vahatra_poly <- calc_indicators(x = Vahatra_poly,\n  #                                 indicators = \"treecover_area\", \n  #                                 min_cover = 30, min_size = 1)\n  \n  save(Vahatra_poly, file = \"data/Vahatra_poly.rds\")\n}\n\n\nMapme produit des colonnes imbriquées pour chaque observation, car dans bien des cas, on peut avoir plusieurs valeurs (par année) pour une même observation, voire plusieurs variables (par exemple, le calcul de l’indicateur traveltime produit des estimations de distance par rapport à une ville pour plusieurs tailles de ville possible. Lorsqu’on spécifie une taille, il produit deux variables : la distance estimée et la taille de la ville prise en compte pour l’estimation.\nCette imbrication n’est pas indispensable pour les trois variables calculées ici (indice de terrain accidenté, distance à une ville et altitude), car on ne cherche qu’une valeur par observation. On va donc dés-imbriquer les variables.\n\n\nCode\n# Valeur agrégées par AP (moyennes pondérées par la surface)\nVahatra_vars_terrain <- Vahatra_poly %>%\n  unnest(cols = c(tri, elevation, traveltime)) %>%\n  st_drop_geometry() %>%\n  select(nom, hectares, indice_accidente = tri_mean, dist_ville = minutes_mean, \n         altitude = elevation_mean) %>%\n  group_by(nom) %>%\n  summarise(indice_accidente = weighted.mean(indice_accidente, hectares,\n                                             na.rm = TRUE),\n            dist_ville = weighted.mean(dist_ville, hectares,\n                                       na.rm = TRUE),\n            altitude = weighted.mean(altitude, hectares,\n                                     na.rm = TRUE))\n# Valeurs qu'on insère dans le jeu de données de travail\nload(\"data/ch1_AP_Vahatra.rds\")\nAP_Vahatra <- AP_Vahatra %>%\n  left_join(Vahatra_vars_terrain, by = \"nom\")\n\nsave(AP_Vahatra, file = \"data/ch2_AP_Vahatra.rds\")\n\n\nOn doit aussi se rappeler que les aires protégées sont parfois composées de plusieurs polygones disjoints et que mapme.biodiversity a calculé chaque indicateur pour chaque polygone séparément. Pour chaque aire protégée, on va donc faire la moyenne de ces indicateurs, pondérée par la surface respective de chaque polygone.\nDonnées d’accessibilité : attention car elles présentent un possible biais d’endogénéité. La construction de route au cours des dernières décennies peut être lié à l’établissement ou non d’aires protégées. L’inclusion d’une variable de contrôle qui peut être en partie affectée par notre variable de traitement (la conservation) est susceptible de problème. Il existe une carte de 2000 qui pourrait être mobilisée :\nOn notera que plusieurs autres indicateurs peuvent être calculés à partir du pabkage mapme.biodiversity:\n\nactive_fire_counts: Calculate active fire counts based on NASA FIRMS polygonsactive_fire_properties: Calculate active fire properties based on NASA FIRMS polygons\nbiome: Calculate biomes statistics (TEOW) based on WWF\ndrought_indicator: Calculate drought indicator statistics\necoregion: Calculate terrestrial ecoregions statistics (TEOW) based on WWF\nlandcover: Calculate area of different landcover classes\nmangroves_area: Calculate mangrove extent based on Global Mangrove Watch (GMW)\npopulation_count: Calculate population count statistics (Worldpop)\nprecipitation_chirps: Calculate precipitation statistics based on CHIRPS\nprecipitation_wc: Calculate precipitation statistics\nsoilproperties: Calculate Zonal Soil Properties\ntemperature_max_wc: Calculate maximum temperature statistics\ntemperature_min_wc: Calculate minimum temperature statistics based on WorldClim\ntraveltime: Calculate accessibility statistics\ntreecover_area: Calculate treecover statistics\ntreecover_area_and_emissions: Calculate treeloss statistics\ntreecoverloss_emissions: Calculate emission statistics\ntri: Calculate Terrain Ruggedness Index (TRI) statistics"
  },
  {
    "objectID": "03-donnees_deforestation.html",
    "href": "03-donnees_deforestation.html",
    "title": "3  Couvert forestier",
    "section": "",
    "text": "Pour commencer, on récupère le travail réalisé par Carvalho et al. (2020) qui complète les informations physiques de Goodman et al. (2018) avec des données relatives au couvert forestier en 1996, 2006 et 2016 et la diversité d’espèces présentes.\n\n\nCode\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(sf)\n\n# Voir le chapitre \"Fondamentaux R\" pour une aide à l'import.\nsup2 <- read_xlsx(\"data/Carvalho2018sup2.xlsx\", # Enplacement du fichier\n                  skip = 8, # Premières lignes du tableau excel à ne pas lire\n                  n_max = 101,  # on ne lit pas les dernières lignes (notes)\n                  col_types = c(\"text\", \"text\", \"text\", \"text\", \n        \"numeric\", \"numeric\", \"numeric\", \"numeric\", \"numeric\", \"numeric\", \n        \"numeric\", \"numeric\", \"numeric\", \"numeric\"))\nsup4 <- read_xlsx(\"data/Carvalho2018sup4.xlsx\", skip = 6,\n                  col_types = c(\"text\", \"numeric\", \"text\", \"numeric\", \"numeric\", \n        \"numeric\", \"numeric\", \"numeric\", \"numeric\", \"numeric\"))\n        \n\n# Carvalho et al. 2008 document in their supp. material 2: \"The three parcels that made up\n# Andohahela (Parcels I, II and III) comprised different types of dominant vegetation and\n# associated animal species, and were exposed to distinct pressures. Andohahela was analysed\n# in its entirety (site number 57), as well as separated\"\n\nsup2 <- sup2 %>% \n  mutate(PA = recode(PA, `Andohahela complete` = \"Andohahela\"),\n         num_atlas_ = as.integer(`Site number`))\n\nsup4 <- sup4 %>%\n  filter(`Habitat type` == \"TOTAL\") %>%\n  mutate(num_atlas_ = as.numeric(Parcel))\n\n\nload(\"data/ch2_AP_Vahatra.rds\")\n\nAP_Vahatra <- AP_Vahatra %>%\n  left_join(sup2, by = \"num_atlas_\") %>%\n  relocate(PA, .after = nom) %>%\n  left_join(sup4, by = \"num_atlas_\")\n\n\nOn complète cette information avec des données de couvert forestier."
  },
  {
    "objectID": "03-donnees_deforestation.html#mapme-exemple-gfc",
    "href": "03-donnees_deforestation.html#mapme-exemple-gfc",
    "title": "3  Couvert forestier",
    "section": "3.2 Mapme (exemple GFC)",
    "text": "3.2 Mapme (exemple GFC)\nLa procédure de traitement de ces fichiers sur Mapme est analogue à celle employée dans la section Chapter 2.\n\n\nCode\n# On charge les polygones travaillés au chapitre 2\nload(\"data/Vahatra_poly.rds\")\n\n# Constitution d'un portefeuille (voir la documentation)\nAP_poly <- init_portfolio(x = AP_poly, \n                          years = 2000:2020,\n                          outdir = \"data_s3/mapme\",\n                          cores = 16,\n                          add_resources = TRUE,\n                          verbose = TRUE)\n\n\n# Get GFW data\nAP_poly <- get_resources(x = AP_poly, \n                         resources = c(\"gfw_treecover\", \"gfw_lossyear\", \n                                       \"gfw_emissions\"))\n\n# Données d'accessibilité de Nelson et al. (2018)\nAP_poly <-  get_resources(x = AP_poly, resource = \"nelson_et_al\",  \n                          range_traveltime = \"5k_110mio\")\n# Données de qualité des sols (uniquement teneur )\nAP_poly  <-  get_resources(x = AP_poly,\n                           resources = \"soilgrids\",  layers = \"clay\", \n                           depths = \"5-15cm\", stats = \"mean\")\n# Modèle numérique de terrain SRTM de la NASA\nAP_poly <- get_resources(x = AP_poly, resource = \"nasa_srtm\")\n# Données de feux\nAP_poly <- get_resources(x = AP_poly, resource = \"nasa_firms\",\n                         instrument = \"MODIS\")\n\n# Get \n# Indicateurs de couvert forestier\nAP_poly  <- calc_indicators(x = AP_poly,\n                            indicators = \"treecover_area_and_emissions\", \n                            min_cover = 30, min_size = 1)\n\n# On réorganise les variables -----------------------------------------------\n# Couvert forestier annuel en colonnes plutôt qu'en liste imbriquée  \ndeforest_par_an <- AP_poly %>%\n  unnest(treecover_area_and_emissions) %>%\n  select(assetid, nom, years, treecover) %>%\n  filter(!is.na(years)) %>%\n  pivot_wider(names_from = \"years\", values_from = \"treecover\", \n              names_prefix = \"treecover_\") %>%\n  st_drop_geometry()\n\n# Jointure du couvert forestier en colonne avec les données initiales\nAP_poly <- AP_poly %>%\n  select(-treecover_area_and_emissions) %>%\n  left_join(deforest_par_an, by = \"assetid\")\n\n\nToutefois, en raison d’un problème liés à la gestion des calculs volumineux, les calculs pour certaines aires protégées renvoient des données aberrantes. Ce point sera mis à jour dans ce guide dès la résolution des erreurs rencontrées.\nA ce stade on se concentrera donc sur les données de TFM présentées plus bas."
  },
  {
    "objectID": "03-donnees_deforestation.html#google-earth-engine-exemple-gfc",
    "href": "03-donnees_deforestation.html#google-earth-engine-exemple-gfc",
    "title": "3  Couvert forestier",
    "section": "3.3 Google Earth Engine (exemple GFC)",
    "text": "3.3 Google Earth Engine (exemple GFC)\nLa plateforme Google Earth Engine est un outil particulièrement pratique et performant pour mobiliser et traiter des données satellitaires. Google Earth Engine peut être utilisé :\n\nen interrogeant son API, et notamment :\n\nen python, avec la librairie gee permet d’interroger l’API de Google Earth Engine.\nen R, au travers de la librairie rgee. Cette dernière est relativement facile d’usage, mais elle est difficile à configurer. Pour aller plus loin : https://r-earthengine.com/rgeebook/\n\ndirectement sur la plateforme https://code.earthengine.google.com/\n\nLa consolde de codage de Google Earth Engine prend la forme suivante :\n\n\n\nDiagramme des composants de la console Google Earth Engine\n\n\nLe langage utilisé sur cet interface est du Javascript. Ci-dessous un exemple de code qui génère les surface (en hectares) de perte de couvert forestier. Pour fonctionner, ce code doit être collé dans un script sur la plateforme Google Earth Engine lancé en cliquant sur “Run”, puis en cliquant sur “Tasks” pour exécuter le code.\n\n\nCode\n scale = 30\n    \n    // PREPARE DATA\n    //look at tree cover, find the area\n    var treeCover = gfc2021.select(['treecover2000']);\n    var areaCover = treeCover.multiply(ee.Image.pixelArea())\n                    .divide(10000).select([0],[\"areacover\"])\n    // total loss area\n    var loss = gfc2021.select(['loss']);\n    var areaLoss = loss.gt(0).multiply(ee.Image.pixelArea())\n                   .divide(10000).select([0],[\"arealoss\"]);\n    // total gain area\n    var gain = gfc2021.select(['gain'])\n    var areaGain = gain.gt(0).multiply(ee.Image.pixelArea())\n                   .divide(10000).select([0],[\"areagain\"]);\n    // final image\n    var total = gfc2021.addBands(areaCover)\n                .addBands(areaLoss)\n                .addBands(areaGain)\n\n    // TOTAL COVER\n    // Map cover area per feature\n    var districtSums = areaCover.reduceRegions({\n      collection: testgu,\n      reducer: ee.Reducer.sum(),\n      scale: scale,\n    });         \n    \n    var addVar = function(feature) {\n\n      // function to iterate over the sequence of years\n      var addVarYear = function(year, feat) {\n        // cast var\n        year = ee.Number(year).toInt()\n        feat = ee.Feature(feat)\n\n        // actual year to write as property\n        var actual_year = ee.Number(2000).add(year)\n\n        // filter year:\n        // 1st: get mask\n        var filtered = total.select(\"lossyear\").eq(year)\n        // 2nd: apply mask\n        filtered = total.updateMask(filtered)\n\n        // reduce variables over the feature\n        var reduc = filtered.reduceRegion({\n          geometry: feature.geometry(),\n          reducer: ee.Reducer.sum(),\n          scale: scale,\n          maxPixels: 1e9\n        })\n\n        // get results\n        var loss = ee.Number(reduc.get(\"arealoss\"))\n        var gain = ee.Number(reduc.get(\"areagain\"))\n\n        // set names\n        var nameloss = ee.String(\"loss_\").cat(actual_year)\n        var namegain = ee.String(\"gain_\").cat(actual_year)\n\n        // set properties to the feature\n        return feat.set(nameloss, loss, namegain, gain)\n      }\n\n      // iterate over the sequence\n      var newfeat = ee.Feature(years.iterate(addVarYear, feature));\n\n      // return feature with new properties\n      return newfeat\n    }\n\n    // Map over the FeatureCollection\n    var areas = districtSums.map(addVar);\n    \n    // Export PA deforestation to a CSV file.\n    Export.table.toDrive({\n      collection: areas,\n      description: 'forest_loss_WDPA_Madagascar',\n      fileFormat: 'CSV'\n    });"
  },
  {
    "objectID": "03-donnees_deforestation.html#python-exemple-tmf",
    "href": "03-donnees_deforestation.html#python-exemple-tmf",
    "title": "3  Couvert forestier",
    "section": "3.4 Python (exemple TMF)",
    "text": "3.4 Python (exemple TMF)\nFichiers préparés en python (code à venir), directement sur les rasters.\n\n\nCode\n# On charge les fichiers préparés par Marc en python (contient 2 feuilles)\ntableur_tmf <- \"data/TMFchangeYear_AP_Vahatra.xlsx\"\n# On commence par charger la feuille déforestation\ntmf_vahatra_defor <- read_excel(tableur_tmf,\n                              sheet = \"TMFdeforestationYear\") %>%\n  select(nom, starts_with(\"TMF\")) # on ne garde que les variables pertinentes\n# On fait ensuite de même avec la feuille dégradation\ntmf_vahatra_degrad <- read_excel(tableur_tmf,\n                              sheet = \"TMFdegradationYear\") %>%\n  select(nom, starts_with(\"TMF\")) # Onn ne garde que les feuilles pertinentes\n\nAP_Vahatra <- AP_Vahatra %>%\n  left_join(tmf_vahatra_degrad, by = \"nom\") %>%\n  left_join(tmf_vahatra_defor, by = \"nom\") \n\n\nTMF_ratios <- AP_Vahatra %>%\n  st_drop_geometry() %>%\n  select(nom, starts_with(\"Forest cover\"), starts_with(\"TMF\")) %>%\n  pivot_longer(cols = starts_with(\"TMF\"), \n               names_to = \"variable\", \n               values_to = \"surface_ha\") %>%\n  mutate(an_valeur = str_extract(variable, \"[:digit:]{4}\"),\n         an_valeur = as.numeric(an_valeur),\n         surface_ratio = case_when(\n           an_valeur < 2000 ~ surface_ha / `Forest cover (ha) in 2006`,\n           an_valeur > 2009 ~ surface_ha / `Forest cover (ha) in 2016`,\n           TRUE ~ surface_ha / `Forest cover (ha) in 2006`),\n         variable = str_replace(variable, \"HA\", \"ratio\")) %>%\n  select(nom, variable, surface_ratio) %>%\n  pivot_wider(names_from = variable, values_from = surface_ratio) %>%\n  select(nom, starts_with(\"TMF\"))\n\nAP_Vahatra <- AP_Vahatra %>%\n  left_join(TMF_ratios, by = \"nom\")\n\nsave(AP_Vahatra, file = \"data/ch3_AP_Vahatra.rds\")"
  },
  {
    "objectID": "03-donnees_deforestation.html#alternatives",
    "href": "03-donnees_deforestation.html#alternatives",
    "title": "3  Couvert forestier",
    "section": "3.5 Alternatives",
    "text": "3.5 Alternatives\nSi on n’est pas à l’aise avec les outils mentionnés plus haut, l’outil Geoquery d’AidData permet d’obtenir des statistiques par aire administrative. Il est également possible de formuler des demandes spécifiques pour d’autres polygones que des aires administratives au travers d’un formulaire dédié.\n\n\n\n\nCarvalho, Fabio, Kerry A. Brown, Adam D. Gordon, Gabriel U. Yesuf, Marie Jeanne Raherilalao, Achille P. Raselimanana, Voahangy Soarimalala, and Steven M. Goodman. 2020. “Methods for Prioritizing Protected Areas Using Individual and Aggregate Rankings.” Environmental Conservation 47 (2): 113–22. https://doi.org/10.1017/S0376892920000090.\n\n\nGoodman, Steven M., Marie Jeanne Raherilalao, Sébastien Wohlhauser, Jean Clarck N. Rabenandrasana, Herivololona M. Rakotondratsimba, Fanja Andriamialisoa, and Malalarisoa Razafimpahanana. 2018. Les Aires Protégées Terrestres de Madagascar: Leur Histoire, Description Et Biote. Association Vahatra."
  },
  {
    "objectID": "04-donnees_en_mailles.html",
    "href": "04-donnees_en_mailles.html",
    "title": "4  Données en mailles",
    "section": "",
    "text": "Une approche courante consiste à diviser le territoires en mailles, carrées ou en forme d’alvéoles d’abeilles (hexagones), et à calculer des indicateurs pour chacune de ces mailles."
  },
  {
    "objectID": "04-donnees_en_mailles.html#constitution-dun-maillage",
    "href": "04-donnees_en_mailles.html#constitution-dun-maillage",
    "title": "4  Données en mailles",
    "section": "4.1 Constitution d’un maillage",
    "text": "4.1 Constitution d’un maillage\nOn montre ci-dessous comment cette approche fonctionne.\n\n\nCode\nlibrary(tidyverse)\nlibrary(tmap)\nlibrary(sf)\nlibrary(mapme.biodiversity)\nlibrary(tidygeocoder) # pour obtenir les coordo GPS d'un point à partir de son nom\nlibrary(maptiles) # Pour télécharger des fonds de carte\n\n# Taille des titres des cartes\ntaille_titres_cartes = 1\n# Surface des hexagones en km2\ntaille_hex <- 5\n\n# Ce qui suit jusqu'à la commande \"save\" ne s'execute que si le résultat n'a pas\n# déjà été généré lors d'une exécution précédente.\nif (file.exists(\"data/grille_mada_donnees_raster.rds\")) {\n  load(\"data/grille_mada_donnees_raster.rds\")\n} else {\n  \n  # Création d'un maillage du territoire émergé --------------------------------\n  \n  # On crée un cadre autour des aires protégées du pays\n  cadre_autour_mada = st_as_sf(st_as_sfc(st_bbox(aires_prot_mada)))\n  \n  # Cellules de 5km de rayon\n  surface_cellule <- taille_hex * (1e+6)\n  taille_cellule <- 2 * sqrt(surface_cellule / ((3 * sqrt(3) / 2))) * sqrt(3) / 2\n  grille_mada <- st_make_grid(x = cadre_autour_mada,\n                              cellsize = taille_cellule,\n                              square = FALSE)\n  # On découpe la grille pour ne garder que les terres émergées\n  cellules_emergees <- st_intersects(contour_mada, grille_mada) %>%\n    unlist()\n  grille_mada <- grille_mada[sort(cellules_emergees)] %>%\n    st_sf()\n} \n\n\nLe maillage est trop fin pour être visible à l’échelle du pays, mais on peut l’observer en zoomant sur une zone spécifique.\n\n\nCode\nsf_use_s2(TRUE) \n\n# On compte le nombre d'hexagones\nn_hex <- nrow(grille_mada)\n# Carte pour visualiser le résultat ------------------------------------------\n\nif (file.exists(\"data/carte_zoom.rds\")) {\n  load(\"data/carte_zoom.rds\")\n  load(\"data/elements_carte_zoom.rds\")\n} else {\n## Carte de droite : zoom sur une zone spécifique-----------------------------\n# On part d'un dataframe contenant une adresse\nnom_centre_zoom <- \"Maroantsetra\"\nzoom_centre <- data.frame(address = nom_centre_zoom) %>%\n  geocode(address, method = \"osm\") %>% # on retrouve sa localisation xy\n  select(long, lat) %>% # on ne garde que le xy\n  as.numeric() %>% # qu'on passe en format numérique attendu par st_point\n  st_point() %>% # On le spécifie en point\n  st_sfc(crs = \"EPSG:4326\") \n\n# On crée une boîte de 100km \nzoom_boite <- zoom_centre %>% # On repart du centre\n  st_buffer(dist = 50000) %>% # On crée un cercle de 50km de rayon\n  st_make_grid(n = 1) \n\n# On filtre les alvéoles pour ne garder que celles qui sont dans le zoom\ngrille_zoom <- st_intersection(grille_mada, zoom_boite)\n\n# On télécharge un fond de carte pour la carte de droite\nfond_carte_zoom <- get_tiles(zoom_boite, provider = \"Stamen.Terrain\", \n                             zoom = 10, crop = TRUE) \n\nsave(n_hex, nom_centre_zoom, zoom_centre, zoom_boite, grille_zoom, \n     fond_carte_zoom, file = \"data/elements_carte_zoom.rds\")\n\n# On était restés en mode interactif, on repasse en mode statique pour les \n# cartes\ntmap_mode(\"plot\")\n\n# On génère la carte de droite\ncarte_zoom <- tm_shape(fond_carte_zoom) + \n  tm_rgb() +\n  tm_shape(grille_zoom) +\n  tm_borders() +\n  tm_shape(zoom_boite) + \n  tm_borders(col = \"red\") +\n  tm_layout(frame = FALSE,\n            main.title = paste(\"Zoom sur la zone de\", nom_centre_zoom),\n            main.title.size = taille_titres_cartes) +\n  tm_credits(get_credit(\"Stamen.Toner\"),\n             bg.color = \"white\",\n             align = \"right\",\n             position = c(\"right\", \"BOTTOM\"))\nsave(carte_zoom, zoom_boite, file = \"data/carte_zoom.rds\")\n}\n\n\n## Carte de gauche : simple à réaliser mais hexagones non visibles -------------\nload(\"data/contour_mada.rds\")\ncarte_grille <- tm_shape(contour_mada) +\n  tm_polygons(col = \"grey\") + \n  tm_shape(zoom_boite) +\n  tm_borders(col = \"red\") +\n  tm_layout(frame = FALSE) +\n  tm_layout(main.title = paste(\"Découpage en\", n_hex,\n                               \"hexagones de\", taille_hex, \"km2\"),\n            main.title.size = taille_titres_cartes)"
  },
  {
    "objectID": "04-donnees_en_mailles.html#récupération-des-données-pour-le-maillage",
    "href": "04-donnees_en_mailles.html#récupération-des-données-pour-le-maillage",
    "title": "4  Données en mailles",
    "section": "4.2 Récupération des données pour le maillage",
    "text": "4.2 Récupération des données pour le maillage\n\n\nCode\nif (file.exists(\"data/grille_mada_donnees_raster.rds\")) {\n  load(\"data/grille_mada_donnees_raster.rds\")\n} else {\n  \n  # Traitement des données satellitaires avec {mapme.bidiversity}---------------\n  \n  # Constitution d'un portefeuille (voir la documentation)\n  grille_mada <- init_portfolio(x = grille_mada, \n                                years = 2000:2020,\n                                outdir = \"data_s3/mapme\",\n                                cores = 24,\n                                add_resources = TRUE,\n                                verbose = TRUE)\n  \n  # Acquisition des données satellitaires requises (rasters) ------------------- \n  # Données d'accessibilité de Nelson et al. (2018)\n  grille_mada <-  get_resources(x = grille_mada, resource = \"nelson_et_al\",  \n                                range_traveltime = \"5k_110mio\")\n  # Données de qualité des sols (uniquement teneur )\n  grille_mada <-  get_resources(x = grille_mada,\n                                resources = \"soilgrids\",  layers = \"clay\", \n                                depths = \"5-15cm\", stats = \"mean\")\n  # Données sur le couvert forestier de Global Forest Watch\n  grille_mada <- get_resources(x = grille_mada, \n                               resources = c(\"gfw_treecover\", \"gfw_lossyear\", \n                                             \"gfw_emissions\"))\n  # Modèle numérique de terrain SRTM de la NASA\n  grille_mada <- get_resources(x = grille_mada, resource = \"nasa_srtm\")\n  # Données de feux\n  grille_mada <- get_resources(x = grille_mada, resource = \"nasa_firms\",\n                               instrument = \"MODIS\")\n  \n  # Calcul des indicateurs -----------------------------------------------------\n  \n  # Indicateurs d'accessibilité\n  grille_mada <- calc_indicators(x = grille_mada,\n                                 \"traveltime\",  stats_accessibility = \"mean\",\n                                 engine = \"extract\")\n  # Indicateurs de sols\n  \n  grille_mada <- calc_indicators(x = grille_mada,\n                                 \"soilproperties\", stats_soil = \"mean\", \n                                 engine = \"extract\")\n \n   # Indicateurs de couvert forestier\n  grille_mada <- calc_indicators(x = grille_mada,\n                                 indicators = \"treecover_area_and_emissions\", \n                                 min_cover = 30, min_size = 1)\n  # Indicateurs de relief de terrain\n  grille_mada <- calc_indicators(x = grille_mada,\n                               indicators = c(\"tri\", \"elevation\"),\n                               stats_tri = \"mean\", stats_elevation = \"mean\")\n  # Indicateurs d'incendies\n  grille_mada <- calc_indicators(x = grille_mada,\n                                 \"active_fire_counts\")\n  grille_mada <- calc_indicators(x = grille_mada,\n                                 \"active_fire_properties\")\n  \n  # Sauvegarde du résultat\n  save(grille_mada, file = \"data_s3/grille_mada_donnees_raster.rds\")\n}\n\n# Assemblage des deux cartes ---------------------------------------------------\ntmap_arrange(carte_grille, carte_zoom, ncol = 2) \n\n\nOn peut également représenter les différentes valeurs des indicateurs générés à partir des données satellitaires.\n\n\nCode\nif (file.exists(\"data/grille_mada_summary.rds\")) {\n  load(\"data/grille_mada_summary.rds\")\n} else {\n  grille_mada_summary <- grille_mada %>%\n    # On met à plat les données de distance\n    unnest(cols = c(traveltime, soilproperties, tri, elevation),\n           names_repair = \"universal\") %>%\n    select(-distance, -layer, -depth, -stat,  -active_fire_counts, \n           -active_fire_properties) %>%\n    rename(distance_minutes_5k_110mio = minutes_mean, mean_clay_5_15cm = mean) \n  \n  grille_mada_summary <- grille_mada_summary %>%\n    unnest(cols = treecover_area_and_emissions) %>%\n    pivot_wider(names_from = \"years\", values_from = c(\"treecover\", \"emissions\")) %>%\n    mutate(var_treecover = (treecover_2020 - treecover_2000)/treecover_2000,\n           sum_emissions = rowSums(across(starts_with(\"emission\")), na.rm = T)) %>%\n    rename(init_treecover_2000 = treecover_2000) %>% # pour le garder\n    select(-starts_with(\"treecover\"), -starts_with(\"emission\")) %>%\n    rename(treecover_2000 = init_treecover_2000) %>%\n    relocate(geometry, .after = last_col())\n  \n  save(grille_mada_summary, file = \"data_s3/grille_mada_summary.rds\")\n}\n\ncarte_acces <- tm_shape(grille_mada_summary) +\n  tm_fill(\"distance_minutes_5k_110mio\",\n          title = \"Distance ville (>5K hab)\",\n          palette = \"Oranges\",\n          style = \"fisher\",\n          n = 8,\n          legend.hist = TRUE) +\n  tm_layout(legend.outside = TRUE,\n            # legend.title.size = 0.8,\n            # legend.text.size = 0.6,\n            legend.hist.width = 1,\n            legend.hist.height = 1)\n\ncarte_sol <- tm_shape(grille_mada_summary) +\n  tm_fill(\"mean_clay_5_15cm\",\n          title = \"Sol argileux (5-15cm prof)\",\n          palette = \"YlOrBr\",\n          n = 8,\n          legend.hist = TRUE) +\n  tm_layout(legend.outside = TRUE,\n            # legend.title.size = 0.8,\n            # legend.text.size = 0.6\n            legend.hist.width = 1,\n            legend.hist.height = 1)\n\ncarte_TRI <- tm_shape(grille_mada_summary) +\n  tm_fill(\"tri_mean\",\n          title = c(\"Terrain accidenté (TRI)\"),\n          palette = \"Blues\",\n          n = 8,\n          legend.hist = TRUE) +\n  tm_layout(legend.outside = TRUE,\n            # legend.title.size = 0.8,\n            # legend.text.size = 0.6,\n            legend.hist.width = 1,\n            legend.hist.height = 1)\n\ncarte_alt <- tm_shape(grille_mada_summary) +\n  tm_fill(\"elevation_mean\",\n          title = \"Altitude\",\n          palette = \"Purples\",\n          n = 8,\n          legend.hist = TRUE) +\n  tm_layout(legend.outside = TRUE,\n            # legend.title.size = 0.8,\n            # legend.text.size = 0.6,\n            legend.hist.width = 1,\n            legend.hist.height = 1)\n\ncarte_cover <- graph_alt <- tm_shape(grille_mada_summary) +\n  tm_fill(\"treecover_2000\",\n          title = \"Couvert arboré en 2000\",\n          palette = \"Greens\",\n          n = 8,\n          legend.hist = TRUE) +\n  tm_layout(legend.outside = TRUE,\n            # legend.title.size = 0.8,\n            # legend.text.size = 0.6,\n            legend.hist.width = 1,\n            legend.hist.height = 1)\n\ncarte_loss <- graph_alt <- tm_shape(grille_mada_summary) +\n  tm_fill(\"var_treecover\",\n          title = \"Perte couvert (2000-2020)\",\n          palette = \"Reds\",\n          n = 8,\n          legend.hist = TRUE) +\n  tm_layout(legend.outside = TRUE,\n            # legend.title.size = 0.8,\n            # legend.text.size = 0.6,\n            legend.hist.width = 1,\n            legend.hist.height = 1)\n\ntmap_arrange(carte_acces, carte_sol, \n             carte_alt, carte_TRI, \n             carte_cover, carte_loss,\n             ncol = 2, nrow = 3) \n\n\n\n\n\nCes approches permettent de mieux capter la variabilité des relations entre les variables spatiales. Elle est utile si les estimations qu’on va ensuite réaliser se font aussi au niveau des aires."
  },
  {
    "objectID": "04-donnees_en_mailles.html#croisement-des-données-daires-protégées-et-satellitaires",
    "href": "04-donnees_en_mailles.html#croisement-des-données-daires-protégées-et-satellitaires",
    "title": "4  Données en mailles",
    "section": "4.3 Croisement des données d’aires protégées et satellitaires",
    "text": "4.3 Croisement des données d’aires protégées et satellitaires\nOn peut maintenant associer les données d’aires protégées aux hexagones afin de les croiser avec les indicateurs issus des données satellitaires déjà calculés pour ces hexagones.\n\n\nCode\nif (file.exists(\"data/grille_mada_summary_AP.rds\")) {\n  load(\"data/grille_mada_summary_AP.rds\")\n} else {\n  # Le code suivant va asocier les hexagones aux aires protégées en se référant\n  # aux AP par leur rang dans la table des AP. On voudra plutôt leur identifiant, \n  # alors on crée une table d'équivalence rang/identifiant \n  aires_prot_mada_rang_id <- aires_prot_mada %>%\n    st_drop_geometry() %>% # Enlève l'information spatiale\n    mutate(AP_ligne = row_number()) %>% # Intègre le numéro de ligne dans un champ\n    select(AP_ligne, WDPAID) # On ne garde que le numéro de ligne et l'identifiant\n  \n  # Pour chaque hexagone, on va maintenant identifier s'ils touchent (\"intersect\")\n  # ou s'ils sont strictiement inclus dans (\"within\") une aire protégé\n  grille_mada_summary_AP <- grille_mada_summary %>%\n    st_transform(crs = mon_scr) %>%\n    mutate(AP_ligne = st_intersects(., aires_prot_mada), # liste des n° de lignes d'AP qui recoupent\n           AP_ligne = map(AP_ligne, 1), # On extrait le 1° élément de la liste (toutes n'ont qu'1 élément)\n           AP_ligne = as.integer(as.character(AP_ligne))) %>%  # formattage en numérique\n    left_join(aires_prot_mada_rang_id, by = \"AP_ligne\") %>% # récupère l'id de l'AP\n    rename(WDPAID_touche = WDPAID) %>% # on renomme pour différentier\n    mutate(AP_ligne = st_within(., aires_prot_mada),\n           AP_ligne = map(AP_ligne, 1),\n           AP_ligne = as.integer(as.character(AP_ligne))) %>%\n    left_join(aires_prot_mada_rang_id, by = \"AP_ligne\") %>%\n    rename(WDPAID_inclus = WDPAID) %>%\n    select(-AP_ligne) \n  \n  grille_mada_summary_AP <- grille_mada_summary_AP %>%\n    st_sf() %>%\n    mutate(position_ap = ifelse(is.na(WDPAID_touche), \"Extérieur\",\n                                ifelse(!is.na(WDPAID_inclus), \"Intérieur\",\n                                       \"Frontière\"))) %>%\n    relocate(geometry, .after = last_col()) \n  save(grille_mada_summary_AP, file = \"data_s3/grille_mada_summary_AP.rds\")\n}\n\n# Une vue après classification\ntm_shape(grille_mada_summary_AP) +\n  tm_fill(col = \"position_ap\", title = \"par rapport aux aires protégées\") +\n  tm_layout(main.title = \"Localisation des hexagones\",\n            # NB : position en minuscules pour laisser un espace avec la marge\n            main.title.position = c(\"center\", \"top\"),\n            main.title.size = taille_titres_cartes,\n            legend.position = c(\"left\", \"top\"),\n            legend.outside = FALSE)\n\n\n\n\n\nLa suite de l’analyse au chapitre Chapter 8"
  },
  {
    "objectID": "05-assignation_aleatoire.html",
    "href": "05-assignation_aleatoire.html",
    "title": "6  Méthode randomisée",
    "section": "",
    "text": "ATTENTION : Il va de soi que les AP malgaches n’ont à aucun moment été assignées aléatoirement. Lors de cette séquence, on fait “comme si”, pour montrer la manière dont les données sont analysées quand il y a eu assignation aléatoire. On verra en fin de session les limites d’une telle approche et dans les suivantes des manières de construire des contrefactuels plus vraisemblables pour un sujet comme celui-ci.\n\n\n\nCode\n# On charge les librairies utiles pour cette analyse\nlibrary(tidyverse) # Facilite la manipulation de données\nlibrary(gt) # Aide à formater de jolis tableaux de rendu\nlibrary(broom) # Aide à formater les rendus de régressions\nlibrary(stargazer) # idem\nlibrary(sf) # Pour les données spatiales\nlibrary(lubridate) # Pour gérer des dates\n\n# Désactiver les notations scientifiques\noptions(scipen =999)\n\nload(\"data/ch3_AP_Vahatra.rds\")\n\nrct_AP_Mada <- AP_Vahatra %>%\n  st_drop_geometry() %>%\n  rename(`Déforestation 1996-2006 (%)` = \n           `Forest loss (ha) between 1996-2006 (percent loss)`,\n         `Déforestation 2006-2016 (%)` = \n           `Forest loss (ha) between 2006-2016 (percent loss)`,\n         `Surface (ha)` = hectares) %>%\n  mutate(Groupe = ifelse(year(date_creation) < 2015, \"Traitement\", \"Controle\"),\n         `Couvert forestier en 1996 (%)` = `Forest cover (ha) in 1996` / \n                                              `Surface (ha)` * 100,\n         `Déforestation 1996-2016 (%)` = \n           (`Forest loss (ha) between 1996-2006 (absolute loss)` + \n             `Forest loss (ha) between 2006-2016 (absolute loss)`) /\n           `Forest cover (ha) in 1996` * 100)\n\n# On fait une série de tests de comparaison de moyenne\nt_tests <- rct_AP_Mada %>% \n  # On applique aux variables de déforestation, couvert en 96 et taille\n  summarise(across(ends_with(\"(%)\") | ends_with(\"(ha)\"),# toutes finissent ainsi\n                   ~ t.test(.[Groupe == \"Controle\"], # on applique un t.test\n                            .[Groupe == \"Traitement\"])$p.value)) %>%\n  mutate(Groupe = \"t-test\")\n\nequilibre_avant <- rct_AP_Mada %>%\n  group_by(Groupe) %>%\n  summarise(`Nombre d'aires` = n(),\n            `Sans forêt` = sum(is.na(`Couvert forestier en 1996 (%)`)), \n            `Surface (ha)` = mean(`Surface (ha)`),\n            `Couvert forestier en 1996 (%)` = \n              mean(`Couvert forestier en 1996 (%)`, na.rm = TRUE)) %>%\n  bind_rows(t_tests) %>% # On colle tous les t-tests \n  mutate(across(!Groupe, ~round(., 2))) %>%# arrondit tout sauf colonne \"Groupe\"\n  select(-starts_with(\"Déforestation\"))# On ne garde que les t-tests de baseline\n\n# Ce qui suit est une série d'opération pour formater le rendu en tableau\nequilibre_avant %>%\n  t() %>% # On transpose lignes <=> colonnes\n  as.data.frame() %>% # La transposition a altéré le format, on remet en tableau\n  tibble::rownames_to_column() %>% # On met le nom des lignes en 1° colonne\n  # \"Truc pour renommer avec le contenu de la première ligne\n  `colnames<-` (filter(., row_number() == 1)) %>% \n  filter(row_number() != 1)%>% # Enlève la 1° ligne qui est maintenant en entête\n  gt() %>%\n  tab_header(title = \"Equilibre des variables avant intervention\",\n             subtitle = \"(exercice : \\\"comme si\\\" c'était une RCT)\") %>%\n  tab_source_note(\"Source : Association Vahatra et Carvalho et al. 2018\")\n\n\n\n\n\n\n  \n    \n      Equilibre des variables avant intervention\n    \n    \n      (exercice : \"comme si\" c'était une RCT)\n    \n  \n  \n    \n      Groupe\n      Controle\n      Traitement\n      t-test\n    \n  \n  \n    Nombre d'aires\n53\n45\nNA\n    Sans forêt\n 4\n 0\nNA\n    Surface (ha)\n66308.62\n63513.89\n    0.88\n    Couvert forestier en 1996 (%)\n54.31\n62.24\n 0.14\n  \n  \n    \n      Source : Association Vahatra et Carvalho et al. 2018\n    \n  \n  \n\n\n\n\nOn a à première vue des équilibres limités “avant intervention”. En moyenne, les deux groupes sont assez proches en termes de surface et de couvert forestier et le test de Student ne permet pas de rejeter l’hypothèse nulle.\nOn va maintenant s’intéresser aux différences de déforestation observées “après intervention” dans le groupe de traitement.\n\n\nCode\ncomparaison_apres <- rct_AP_Mada %>%\n  group_by(Groupe) %>%\n  summarise(across(starts_with(\"Déforestation\"), ~ mean(., na.rm = TRUE))) %>%\n  bind_rows(t_tests) %>% # On colle tous les t-tests \n  mutate(across(!Groupe, ~round(., 2))) %>%# arrondit tout sauf colonne \"Groupe\"\n  select(Groupe, starts_with(\"Déforestation\"))# On ne garde que les t-tests de baseline\n\n\n# Même procédure que plus haut pour formater le rendu en tableau\ncomparaison_apres  %>%\n  t() %>% # On transpose lignes <=> colonnes\n  as.data.frame() %>% # La transposition a altéré le format, on remet en tableau\n  tibble::rownames_to_column() %>% # On met le nom des lignes en 1° colonne\n  # \"Truc pour renommer avec le contenu de la première ligne\n  `colnames<-` (filter(., row_number() == 1)) %>% \n  filter(row_number() != 1)%>% # Enlève la 1° ligne qui est maintenant en entête\n  gt() %>%\n  tab_header(title = \"Moyennes après intervention\",\n             subtitle = \"(exercice : \\\"comme si\\\" c'était une RCT)\") %>%\n  tab_source_note(\"Source : Association Vahatra et Carvalho et al. 2018\")\n\n\n\n\n\n\n  \n    \n      Moyennes après intervention\n    \n    \n      (exercice : \"comme si\" c'était une RCT)\n    \n  \n  \n    \n      Groupe\n      Controle\n      Traitement\n      t-test\n    \n  \n  \n    Déforestation 1996-2006 (%)\n8.21\n4.12\n0.07\n    Déforestation 2006-2016 (%)\n8.50\n3.91\n0.01\n    Déforestation 1996-2016 (%)\n15.90\n 7.87\n 0.00\n  \n  \n    \n      Source : Association Vahatra et Carvalho et al. 2018\n    \n  \n  \n\n\n\n\nOn peut également réaliser une régression simple, qu’on présente selon le format courant pour la littérature en économie grâce au package {stargazer} (Hlavac 2022).\n\n\nCode\ndef_96_06 <- lm(`Déforestation 1996-2006 (%)`  ~ Groupe, data = rct_AP_Mada)\ndef_06_16 <- lm(`Déforestation 2006-2016 (%)`  ~ Groupe, data = rct_AP_Mada)\ndef_96_16 <- lm(`Déforestation 1996-2016 (%)`  ~ Groupe, data = rct_AP_Mada)\n\n# tidy(def_96_06) %>%\n#   gt() %>%\n#   tab_header(title = \"Déforestation 1996-2006 (%)\",\n#              subtitle = \"(exercice : \\\"comme si\\\" c'était une RCT)\") %>%\n#   tab_source_note(\"Source : Association Vahatra et Carvalho et al. 2018\")\n# \n# tidy(def_06_16) %>%\n#   gt() %>%\n#   tab_header(title = \"Déforestation 2006-2016 (%)\",\n#              subtitle = \"(exercice : \\\"comme si\\\" c'était une RCT)\") %>%\n#   tab_source_note(\"Source : Association Vahatra et Carvalho et al. 2018\")\n# \n# tidy(def_96_16) %>%\n#   gt() %>%\n#   tab_header(title = \"Déforestation 1996-2016 (%)\",\n#              subtitle = \"(exercice : \\\"comme si\\\" c'était une RCT)\") %>%\n#   tab_source_note(\"Source : Association Vahatra et Carvalho et al. 2018\")\n\nstargazer(def_96_06, def_06_16, def_96_16, type = \"text\",\n          title = \"Impact de la conservation sur la perte de couvert forestier\",\n          notes = \"Données : Association Vahatra et Carvalho et al. 2018\")\n\n\n\nImpact de la conservation sur la perte de couvert forestier\n=======================================================================================================================\n                                                                 Dependent variable:                                   \n                              -----------------------------------------------------------------------------------------\n                              `Déforestation 1996-2006 (%)` `Déforestation 2006-2016 (%)` `Déforestation 1996-2016 (%)`\n                                           (1)                           (2)                           (3)             \n-----------------------------------------------------------------------------------------------------------------------\nGroupeTraitement                         -4.092*                      -4.593***                     -8.031***          \n                                         (2.308)                       (1.745)                       (2.831)           \n                                                                                                                       \nConstant                                8.214***                      8.499***                      15.902***          \n                                         (1.597)                       (1.208)                       (1.959)           \n                                                                                                                       \n-----------------------------------------------------------------------------------------------------------------------\nObservations                               94                            94                            94              \nR2                                        0.033                         0.070                         0.080            \nAdjusted R2                               0.023                         0.060                         0.070            \nResidual Std. Error (df = 92)            11.179                         8.454                        13.713            \nF Statistic (df = 1; 92)                 3.143*                       6.924***                      8.046***           \n=======================================================================================================================\nNote:                                                                                       *p<0.1; **p<0.05; ***p<0.01\n                                                                  Données : Association Vahatra et Carvalho et al. 2018\n\n\nAnalyse la relation aux variables topologiques (altitude, indice de terrain accidenté) et de temps de trajet à la ville la plus proche. Le seuil retenu ici pour une ville est toute localité de plus de 5000 habitants.\n\n\nCode\nt_tests_autres <- rct_AP_Mada %>% \n  # On applique aux variables d'altitude, TRI et temps de trajet aux villes.\n  summarise(across(indice_accidente:altitude,# toutes finissent ainsi\n                   ~ t.test(.[Groupe == \"Controle\"], # on applique un t.test\n                            .[Groupe == \"Traitement\"])$p.value)) %>% \n  mutate(Groupe = \"t-test\")\n\n\nequilibre_autres <- rct_AP_Mada %>%\n  group_by(Groupe) %>%\n  summarise(`Nombre d'aires` = n(),\n            indice_accidente = mean(indice_accidente, na.rm = TRUE), \n            dist_ville = mean(dist_ville, na.rm = TRUE),\n            altitude = mean(altitude, na.rm = TRUE)) %>%\n  bind_rows(t_tests_autres) %>% # On colle tous les t-tests \n  mutate(across(!Groupe, ~round(., 2))) %>%# arrondit tout sauf colonne \"Groupe\"\n  select(-starts_with(\"Déforestation\"))# On ne garde que les t-tests de baseline\n\nequilibre_autres %>%\n  gt() %>%\n  tab_header(title = \"Equilibre entre les groupes en matière topologique\",\n             subtitle = \"(exercice : \\\"comme si\\\" c'était une RCT)\") %>%\n  tab_source_note(\"Source : Association Vahatra et Carvalho et al. 2018\")\n\n\n\n\n\n\n  \n    \n      Equilibre entre les groupes en matière topologique\n    \n    \n      (exercice : \"comme si\" c'était une RCT)\n    \n  \n  \n    \n      Groupe\n      Nombre d'aires\n      indice_accidente\n      dist_ville\n      altitude\n    \n  \n  \n    Controle\n53\n11.02\n148.77\n540.45\n    Traitement\n45\n12.08\n233.31\n538.63\n    t-test\nNA\n0.53\n0.03\n0.99\n  \n  \n    \n      Source : Association Vahatra et Carvalho et al. 2018\n    \n  \n  \n\n\n\n\nCode\nsave(rct_AP_Mada, file = \"data/rct_AP_Mada.rds\")\n\n\nLe temps de trajet aux villes est significativement distinct entre les deux groupes. Attention, car cette variable pose un problème d’endogénéité (discussion en séance sur ce point).\n\n\nCode\ndef_96_16_controle <- lm(`Déforestation 1996-2016 (%)` ~ \n                           Groupe + dist_ville, \n                         data = rct_AP_Mada)\n\nstargazer(def_96_16, def_96_16_controle, type = \"text\")\n\n\n\n===============================================================\n                                Dependent variable:            \n                    -------------------------------------------\n                           `Déforestation 1996-2016 (%)`       \n                             (1)                   (2)         \n---------------------------------------------------------------\nGroupeTraitement          -8.031***             -6.660**       \n                           (2.831)               (2.958)       \n                                                               \ndist_ville                                       -0.013        \n                                                 (0.008)       \n                                                               \nConstant                  15.902***             18.013***      \n                           (1.959)               (2.341)       \n                                                               \n---------------------------------------------------------------\nObservations                 94                    90          \nR2                          0.080                 0.103        \nAdjusted R2                 0.070                 0.082        \nResidual Std. Error   13.713 (df = 92)      13.669 (df = 87)   \nF Statistic         8.046*** (df = 1; 92) 4.989*** (df = 2; 87)\n===============================================================\nNote:                               *p<0.1; **p<0.05; ***p<0.01\n\n\nApparemment, le traitement reste significatif une fois que l’on contrôle pour la distance aux villes.\n\n\n\n\nHlavac, Marek. 2022. Stargazer: Well-Formatted Regression and Summary Statistics Tables. https://CRAN.R-project.org/package=stargazer."
  },
  {
    "objectID": "06-matching_AP.html",
    "href": "06-matching_AP.html",
    "title": "7  Méthode d’appariement",
    "section": "",
    "text": "On a vu dans le chapitre précédent que les comparaisons simples réalisées entre les premières et les dernières aires à avoir été formellement protégées pose problème.\nOn va maintenant chercher à renforcer la comparabilité entre le goupe de traitment et le groupe de contrôle en réalisant un appariemment (cf. diapos de présentation).\nOn va utiliser le package {MatchIt}: ne pas hésiter à se référer à la documentation du package : [TODO: insérer le lien vers la doc]\nOn va commencer par réaliser quelques ajustements, car {MatchIt} requiert qu’aucune valeur des variables mobilisées ne soit manquante. On va donc retirer les observations comportant des NA.\n\n\nCode\nlibrary(tidyverse) # Simplifie la manipulation de données\nlibrary(lubridate) # Simplifie les opérations sur des dates\nlibrary(sf) # Pour traiter les données spatiales\nlibrary(MatchIt) # Pour réaliser les appariements.\nlibrary(cobalt) # Pour les tests d'équilibre sur l'appariement\nlibrary(gt) # Pour faire de jolies tables\nlibrary(stargazer) # Pour préssenter les résultats de régressions\n\n# Désactiver les notations scientifiques\noptions(scipen =999)\n# On recharge les données préparées dans le chapitre 3\nload(\"data/ch3_AP_Vahatra.rds\")\n\n# Harmoniser les données entre avant et après.\nload(\"data/rct_AP_Mada.rds\")\n\n\nrct_AP_Mada_noNA <- rct_AP_Mada %>%\n  # On enlève les observations pour lesquelles il manque des valeurs\n  filter(!is.na(`Déforestation 1996-2016 (%)`)) %>%\n  filter(!is.na(dist_ville)) %>%\n  # La vatiable de traitement doit être recodée en [0, 1]\n  mutate(traitement = ifelse(Groupe == \"Traitement\", 1, 0)) %>%\n  rename(surface_ha = `Surface (ha)`, \n         couv_foret_96 = `Couvert forestier en 1996 (%)`)\n\nrct_AP_Mada_noNA %>%\n  group_by(Groupe) %>%\n  summarize(`Nombre d'aires protégées` = n()) %>%\n  gt() %>%\n  tab_header(\"Observations par groupe avant appariemment\") %>%\n  tab_source_note(\"Source : Association Vahatra et Carvalho et al. 2018\")\n\n\n\n\n\n\n  \n    \n      Observations par groupe avant appariemment\n    \n    \n  \n  \n    \n      Groupe\n      Nombre d'aires protégées\n    \n  \n  \n    Controle\n47\n    Traitement\n43\n  \n  \n    \n      Source : Association Vahatra et Carvalho et al. 2018\n    \n  \n  \n\n\n\n\nPour commencer, on va estimer le un modèle qui estime dans quel mesure la propension pour une aire d’avoir été protégée avant 2015 dépend de sa taille, de son taux de couverture forestière en 1996, de son altitude, de son caractère accidenté et de sa distance d’une ville d’au moins 5000 habitants.\nCette spécification peut se représenter selon l’équation suivante : [TODO: revoir la spécification au format standard]\n\\[\nT = A + B + C + D + E  \n\\]\nOù Y est le traitement, A est la taille (surface en hectares, B le taux de couverture forestière en 1996, C l’altitude, D, le caractère accidenté et E le temps de parcours à une ville d’au moins 5000 habitants.\nCette même formule est encodée en R de la manière suivante :\n\npscor <- traitement ~  surface_ha + \n                       couv_foret_96 + \n                       altitude +\n                       indice_accidente + \n                       dist_ville\n\nOn va maintenant réaliser une régression pour connaître l’influence de ces facteurs dans la désignation des aires comme protégées.\n\n\nCode\n# On ré\nreg_select <- glm(formula = pscor,\n                  family = binomial(link = \"probit\"),\n                  data = rct_AP_Mada_noNA)\n\nstargazer(reg_select, type = \"text\")\n\n\n\n=============================================\n                      Dependent variable:    \n                  ---------------------------\n                          traitement         \n---------------------------------------------\nsurface_ha                 -0.00000          \n                           (0.00000)         \n                                             \ncouv_foret_96                0.005           \n                            (0.006)          \n                                             \naltitude                    -0.0003          \n                           (0.0004)          \n                                             \nindice_accidente             0.004           \n                            (0.023)          \n                                             \ndist_ville                  0.002**          \n                            (0.001)          \n                                             \nConstant                    -0.498           \n                            (0.364)          \n                                             \n---------------------------------------------\nObservations                  90             \nLog Likelihood              -59.054          \nAkaike Inf. Crit.           130.108          \n=============================================\nNote:             *p<0.1; **p<0.05; ***p<0.01\n\n\nOn va maintenant utiliser ce modèle pour comparer les aires protégées traitées en premier par rapport à celles traitées plus récemment.\n\n\nCode\n# Calcul du matching\ndef_96_16_match <- matchit(formula = pscor,\n                           family = binomial(link = \"probit\"),\n                           method = \"nearest\",\n                           discard = \"both\",\n                           replace = FALSE,\n                           distance = \"glm\",\n                           data = rct_AP_Mada_noNA)\n\nprint(def_96_16_match)\n\n\nA matchit object\n - method: 1:1 nearest neighbor matching without replacement\n - distance: Propensity score [common support]\n             - estimated with logistic regression\n - common support: units from both groups dropped\n - number of obs.: 90 (original), 78 (matched)\n - target estimand: ATT\n - covariates: surface_ha, couv_foret_96, altitude, indice_accidente, dist_ville\n\n\nOn peut maintenant observer les équilibres entre les groupes traités et contrôle avant et après l’appariement.\n\n\nCode\nsummary(def_96_16_match)\n\n\n\nCall:\nmatchit(formula = pscor, data = rct_AP_Mada_noNA, method = \"nearest\", \n    distance = \"glm\", discard = \"both\", replace = FALSE, family = binomial(link = \"probit\"))\n\nSummary of Balance for All Data:\n                 Means Treated Means Control Std. Mean Diff. Var. Ratio\ndistance                0.5123        0.4462          0.4600     1.7294\nsurface_ha          64859.7293    71636.6002         -0.0856     0.5216\ncouv_foret_96          61.8218       55.4658          0.2363     1.1522\naltitude              538.6298      475.9134          0.1328     0.7177\nindice_accidente       12.0830       10.7187          0.1667     1.0636\ndist_ville            233.3141      152.0447          0.3811     2.1820\n                 eCDF Mean eCDF Max\ndistance            0.1409   0.2622\nsurface_ha          0.0910   0.2509\ncouv_foret_96       0.0795   0.2425\naltitude            0.0890   0.2103\nindice_accidente    0.0578   0.1390\ndist_ville          0.1333   0.2588\n\n\nSummary of Balance for Matched Data:\n                 Means Treated Means Control Std. Mean Diff. Var. Ratio\ndistance                0.4783        0.4712          0.0497     0.9995\nsurface_ha          67277.5891    56596.5970          0.1348     0.7366\ncouv_foret_96          60.8113       60.4309          0.0141     1.3400\naltitude              518.5203      503.7378          0.0313     0.8548\nindice_accidente       11.6356       11.6417         -0.0007     1.0171\ndist_ville            182.9262      166.4807          0.0771     0.8322\n                 eCDF Mean eCDF Max Std. Pair Dist.\ndistance            0.0379   0.1538          0.0883\nsurface_ha          0.1217   0.2821          1.0327\ncouv_foret_96       0.0473   0.1795          0.8761\naltitude            0.0490   0.1282          1.0065\nindice_accidente    0.0316   0.0769          1.1074\ndist_ville          0.0692   0.2051          0.3803\n\nSample Sizes:\n          Control Treated\nAll            47      43\nMatched        39      39\nUnmatched       6       0\nDiscarded       2       4\n\n\n\nExercice : Etudiez les tables ci-dessus. Quel effet a eu l’appariement sur l’équilibre des variables entre le groupe de traitement et le groupe de contrôle ? Combien d’observation ont été écartées.\n\nOn peut également représenter l’équilibre entre les variables avant et après traitement avec les graphiques suivants.\n\n\nCode\nbal.plot(def_96_16_match, var.name = \"dist_ville\", which = \"both\")\n\n\n\n\n\n\nExercice : Quel effet a eu l’appariement sur la varialbe de distance à la ville ? Les autres variables d’appariement produisent-elles un effet aussi visible ?\n\nLe modèle qu’on utilise pour estimer l’impact est très proche de celui exposé ci-dessus, à la différence que la variable de traitement passe dans la partie droite, et qu’elle est remplacée par la déforestation.\n\\[\nY = T + A + B + C + D + E\n\\]\nOù Y est la déforestation, T est le traitement, A est la taille (surface en hectares, B le taux de couverture forestière en 1996, C l’altitude, D, le caractère accidenté et E le temps de parcours à une ville d’au moins 5000 habitants.\nCette formule est codée en R de la manière suivante :\n\n\nCode\n# On extrait la donnée de l'appariement\n\n#| code-fold: false\nestimp <- `Déforestation 1996-2016 (%)` ~   \n                          traitement +\n                          surface_ha + \n                          couv_foret_96 + \n                          altitude +\n                          indice_accidente + \n                          dist_ville\n\n\nOn va donc réaliser une régression, en tenant compte des pondérations générées par l’algorithme d’appariement (variable “weight”).\n\n\nCode\n# On extrait les données de l'appariement\ndef_96_16_match_data <- match.data(def_96_16_match)\n# On effectue une régression simple avec la formule précédente\ndef_96_16_match_est <- lm(formula = estimp,\n                          data = def_96_16_match_data,\n                          weights = weights)\n# On visualise les résultats\nstargazer(def_96_16_match_est, type = \"text\")\n\n\n\n=================================================\n                         Dependent variable:     \n                    -----------------------------\n                    `Déforestation 1996-2016 (%)`\n-------------------------------------------------\ntraitement                    -6.781**           \n                               (3.166)           \n                                                 \nsurface_ha                     0.00000           \n                              (0.00002)          \n                                                 \ncouv_foret_96                  -0.083            \n                               (0.072)           \n                                                 \naltitude                        0.001            \n                               (0.005)           \n                                                 \nindice_accidente               -0.357            \n                               (0.260)           \n                                                 \ndist_ville                     -0.009            \n                               (0.015)           \n                                                 \nConstant                      25.457***          \n                               (4.826)           \n                                                 \n-------------------------------------------------\nObservations                     78              \nR2                              0.144            \nAdjusted R2                     0.072            \nResidual Std. Error       13.939 (df = 71)       \nF Statistic              1.992* (df = 6; 71)     \n=================================================\nNote:                 *p<0.1; **p<0.05; ***p<0.01\n\n\n\n7.0.1 Exercice simple\nAnalysez, interprétez et critiquez les résultats ci-dessus.\n\n\n7.0.2 Exercice intermédiaire\nAjoutez des variables d’interne et modifiez les paramètres de la fonction de matching.\n\n\n7.0.3 Exercice avancé\nRéalisez une analyse analogue avec les données de déforestation TMF. Rédigez une analyse interprétative."
  },
  {
    "objectID": "07-avant_apres.html",
    "href": "07-avant_apres.html",
    "title": "9  Comparaison avant-après",
    "section": "",
    "text": "Ainsi, nous allons comparer pour chaque AP, les taux de déforestation entre les années qui précèdent la création de l’AP et les années qui suivent la création.\nEn revanche, comme vu dans la partie théorique, cette approche repose sur l’hypothèse que la seule différence entre les périodes est la mise en place de la politique.\nAutrement dit, le seul facteur entre ces différentes périodes qui impactent le taux de dégradation des forêts est la mise en place de l’AP.\nCi-dessous, un tableau représentant les dates de création des aires protégées.\n\n\nCode\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(sf)\nlibrary(gt)\n# Désactiver les notations scientifiques\noptions(scipen =999)\n\n# On reprend les données telles que préparées au chapitre 3\nload(\"data/ch3_AP_Vahatra.rds\")\n\nAP_Vahatra %>%\n  mutate(an_creation = year(date_creation)) %>%\n  select(nom, an_creation) %>%\n  ggplot(aes(x = an_creation)) +\n  geom_rect(xmin = 1995, xmax = 2016, ymin = 0, \n            ymax = 54, fill = \"yellow\", alpha = 0.3) +\n  geom_bar() +\n  xlim(c(NA, 2020)) + \n  geom_vline(xintercept = c(1990, 2020), col = \"red\") +\n  ylim(NA, 53) +\n  ggtitle(\"Dates de création des AP et de disponibilité\\ndes données TMF\")\n\n\n\n\n\nLes données mobilisées sont celles de TMF pour lesquelles on dispose d’un historique allant de 1990 à 2020. On se concentre sur les aires protégées dont le statut a été décrété entre ces deux dates.\nEn revanche, afin d’avoir non pas un unique taux de dégradation du couvert forestier mais une tendance (i.e l’évolution) de celui-ci, nous restreignons notre échantillon aux AP dont le statut a été octroyé entre 1995 et 2015.\nEn effet, si on se focalise uniquement sur le taux de déforestation juste avant et juste après, il se peut qu’un évènement ait impacté le taux de dégradation et ne représente pas réellement la dégradation du couvert forestier dans l’AP.\nPar exemple, si en t-1, il y a eu un énorme feu lié à un évènement naturel, alors nous risquons de surestimer la perte de couvert forestier avant et donc d’inférer à l’AP, un impact beaucoup plus bénéfique que ce qu’il est réellement.\nNotre échantillon final contient 72 AP (sur les 98 initiales de la base de données Vahatra _ en jaune sur le graphique).\nNous normalisons les dates d’octroi du statut d’AP c’est à dire qu’on transforme les années calendaires (1995, 1996,…) en année relative à la mise en place de l’AP.\nPar exemple, si une AP est créée en 2000 (qui correspondra à l’année 0), toutes les autres années seront exprimées relativement à celle-ci et donc, 1995 sera égale à -5 et 2005 à 5.\nCette transformation nous permet de pouvoir visualiser les données. Dans le tableau suivant, nous avons la moyenne annuelle de la surface (exprimée en hectare et en pourcentage) de l’AP qui a été dégradé sur les 5 années précédents et suivants la mise en place de l’AP.\n\n\nCode\n# Une fonction pour créer un jeu avec des dates normalisées\nans_vs_crea <- function(x, vars_commenct_par = \"TMFdeg_HA\",\n                       ans_marge = 5) {\n  avant_apres_abs <- AP_Vahatra %>%\n    st_drop_geometry() %>%\n    select(nom, date_creation, starts_with(vars_commenct_par)) %>%\n    filter(year(date_creation) >= 1995 & year(date_creation) <= 2017) %>%\n    pivot_longer(cols = starts_with(\"TMF\"),\n                 names_to = \"variable\",\n                 values_to = \"valeur\") %>%\n    mutate(an_valeur = str_extract(variable, \"[:digit:]{4}\"),\n           an_valeur = as.numeric(an_valeur),\n           an_creation = year(date_creation),\n           an_val_crea = an_valeur - an_creation,\n           sequence_crea = ifelse(an_val_crea < 0, \"Avant\",\n                                   ifelse(an_val_crea > 0, \"Après\", \"Création\"))) %>%\n    filter(an_val_crea >= ans_marge * -1 & an_val_crea <= ans_marge & an_val_crea != 0) %>%\n    mutate(sequence_crea = factor(sequence_crea, levels = c(\"Avant\", \"Après\")))\n}\n\n# Un jeu avec les dégradations 5 ans avant et 5 ans après\n## en valeur absolue\ndeg_avap_abs <- ans_vs_crea(AP_Vahatra, vars_commenct_par = \"TMFdeg_HA\",\n                       ans_marge = 5)\n## en valeur relative (ratio)\ndeg_avap_rel <- ans_vs_crea(AP_Vahatra, vars_commenct_par = \"TMFdeg_ratio\",\n                       ans_marge = 5)\n\nmoy_deg_avap_abs <- deg_avap_abs %>%\n  group_by(sequence_crea) %>%\n  summarise(`Moyenne sur 5 ans` = mean(valeur, na.rm = TRUE)) %>%\n  mutate(Indicateur = \"Surface en valeur absolue (ha)\")\n\nmoy_deg_avap_rel <- deg_avap_rel %>%\n  group_by(sequence_crea) %>%\n  summarise(`Moyenne sur 5 ans` = mean(valeur, na.rm = TRUE)*100) %>%\n  mutate(Indicateur = \"Surface en valeur relative (%)\")\n\nmoy_deg_avap <-bind_rows(moy_deg_avap_abs, moy_deg_avap_rel) %>%\n  mutate(`Moyenne sur 5 ans` = round(`Moyenne sur 5 ans`, 2)) %>%\n  pivot_wider(names_from = sequence_crea, values_from = `Moyenne sur 5 ans`) \n\ngt(moy_deg_avap) %>% \n  tab_header(title = \"Moyenne de dégradation annuelle sur 5 ans\")  %>% \n  tab_source_note(c(\"Source : TMF, Carvalho et al. 2018 et association Vahatra.\",\n                    \"Calculs des auteurs.\"))\n\n\n\n\n\n\n  \n    \n      Moyenne de dégradation annuelle sur 5 ans\n    \n    \n  \n  \n    \n      Indicateur\n      Avant\n      Après\n    \n  \n  \n    Surface en valeur absolue (ha)\n178.35\n264.25\n    Surface en valeur relative (%)\n0.49\n0.74\n  \n  \n    \n      Source : TMF, Carvalho et al. 2018 et association Vahatra.\n    \n    \n      Calculs des auteurs.\n    \n  \n  \n\n\n\n\nD’après le tableau ci-dessous, quelles conclusions pouvons-nous tirer ?\nOn peut aussi visualiser cette information sur les 10 plus grandes aires.\n\n\nCode\n# A compléter.\n\n\nOn va maintenant réaliser quelques tests statistiques pour analyser la significativité de ces différeces.\n\n\nCode\n# A completer."
  },
  {
    "objectID": "09-bibliographie.html",
    "href": "09-bibliographie.html",
    "title": "References",
    "section": "",
    "text": "Barnier, Julien. 2022. “Introduction à r Et Au Tidyverse.”\nhttps://juba.github.io/tidyverse/index.html.\n\n\nBédécarrats, Florent, and Alexandre Hobeika. 2017. “Une\nAlternative à Word : Écrire En\nRMarkdown.” Billet. Data Sciences Sociales.\nhttp://data.hypotheses.org/1144.\n\n\nCallaway, Brantly, and Pedro H. C. Sant’Anna. 2021a.\n“Difference-in-Differences with Multiple Time Periods.” https://doi.org/10.1016/j.jeconom.2020.12.001.\n\n\n———. 2021b. “Difference-in-Differences with Multiple Time\nPeriods.” Journal of Econometrics, Themed Issue:\nTreatment Effect 1, 225 (2): 200–230. https://doi.org/10.1016/j.jeconom.2020.12.001.\n\n\nCarvalho, Fabio, Kerry A. Brown, Adam D. Gordon, Gabriel U. Yesuf, Marie\nJeanne Raherilalao, Achille P. Raselimanana, Voahangy Soarimalala, and\nSteven M. Goodman. 2020. “Methods for Prioritizing Protected Areas\nUsing Individual and Aggregate Rankings.” Environmental\nConservation 47 (2): 113–22. https://doi.org/10.1017/S0376892920000090.\n\n\nGaliana, Lino, and Olivier Meslin, eds. 2022. “utilitR:\nDocumentation Collaborative Sur r.” https://www.book.utilitr.org/.\n\n\nGoodman, Steven M., Marie Jeanne Raherilalao, Sébastien Wohlhauser, Jean\nClarck N. Rabenandrasana, Herivololona M. Rakotondratsimba, Fanja\nAndriamialisoa, and Malalarisoa Razafimpahanana. 2018. Les Aires\nProtégées Terrestres de Madagascar: Leur Histoire,\nDescription Et Biote. Association Vahatra.\n\n\nGörgen, Darius A., and Om Prakash Bhandari. 2022.\nMapme.biodiversity: Efficient Monitoring of Global Biodiversity\nPortfolios. https://CRAN.R-project.org/package=mapme.biodiversity.\n\n\nGrolemund, Garrett, and Hadley Wickham. 2022. R for Data Science:\nImport, Tidy, Transform, Visualize, and Model Data. 1st edition.\nSebastopol, CA: O’Reilly Media. https://r4ds.had.co.nz/.\n\n\nHlavac, Marek. 2022. Stargazer: Well-Formatted Regression and\nSummary Statistics Tables. https://CRAN.R-project.org/package=stargazer.\n\n\nKluve, Jochen, Johannes Schielain, Melvin Wong, and Yota Eilers. 2022.\n“The KfW Protected Areas\nPortfolio: A Rigorous Impact\nEvaluation.” Frankfürt.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2022.\nGeocomputation with R. Boca Raton London New York: Routledge.\nhttps://geocompr.robinlovelace.net/."
  },
  {
    "objectID": "10-fondamentaux_R.html",
    "href": "10-fondamentaux_R.html",
    "title": "11  Fondamentaux pour l’utilisation de R",
    "section": "",
    "text": "Il existe de plusieurs ressources en français pour apprendre à utiliser R. Nous vous recommandons en particulier :\nLes bonnes ressources anglophones gratuites sont très nombreuses, très facile à trouver sur le Web. Les grands classiques est R for data science, de Grolemund et Wickham (2022). On se focalise ici avec deux autres qui sont le plus en lien avec nos sujets :\nN’hésitez pas à chercher directement sur le Web en cas de problème. Vous serez souvent conduits vers les forums stackoverflow ou RStudio, qui sont aussi des ressources très précieuses pour résoudre des problèmes très spécifiques."
  },
  {
    "objectID": "10-fondamentaux_R.html#installation",
    "href": "10-fondamentaux_R.html#installation",
    "title": "11  Fondamentaux pour l’utilisation de R",
    "section": "11.1 Installation",
    "text": "11.1 Installation\nOn installe R et RStudio :\n\nTélécharger et installer R (page officielle proposant les installateurs et instructions d’installation)\nTélécharger et installer RStudio (page officielle proposant les installateurs et instructions d’installation)\n\n\nA noter : un nombre croissant d’utilisteurs utilise VS Code. C’est une alternative intéressante, pour des utilisateurs déjà confirmés :"
  },
  {
    "objectID": "10-fondamentaux_R.html#import-des-données",
    "href": "10-fondamentaux_R.html#import-des-données",
    "title": "11  Fondamentaux pour l’utilisation de R",
    "section": "11.2 Import des données",
    "text": "11.2 Import des données\nEn très bref :\n\nPour les fichiers excle ou csv, dans le volet “files” du panneau en bas à droite de l’interface Rstudio, cliquer sur le fichier en question et utiliser l’assistant d’import.\nPour les autres fichiers, se référer à l’aide ou chercher sur internet.\n\nVoir cette page pour un topo sur les imports. [#TODO:Préciser l’url]"
  },
  {
    "objectID": "10-fondamentaux_R.html#principes-élémentaires-de-manipulation-de-données-en-r",
    "href": "10-fondamentaux_R.html#principes-élémentaires-de-manipulation-de-données-en-r",
    "title": "11  Fondamentaux pour l’utilisation de R",
    "section": "11.3 Principes élémentaires de manipulation de données en R",
    "text": "11.3 Principes élémentaires de manipulation de données en R\nOn se focalise ici sur quelques aspects qui peuvent être requis pour la manipulation du code et à la marge. Points à traiter :\n\nLe signe <- correspond à l’assignation d’une valeur à une variable. Il est presque équivalent à =, avec quelques différences dans certaines circonstances particulières, qui fait qu’on privilégie toujours <-.\n\n\n# Ce qui suit un dièze n'est pas exécuté. On appelle ça un commentaire.\n\n# On commence par faire une opération simple\n3 + 4\n\n[1] 7\n\n# Ce qui équivaut à :\na <- 3\nb <- 4\na + b\n\n[1] 7\n\n# Et on peut également stocker le résultat dans une nouvelle variable \nc <- a + b\nc\n\n[1] 7\n\n\n\nR est constitué de fonctions. De nombreuses fonctions prédéfinies sont contenues dans la base de R ou dans des packages qu’on ajoute (qu’on verra plus tard). La meilleure manière de comprendre ce qu’est une fonction est d’en créer une soi même.\n\n\n# On crée une fonction \"ajoute\" qui prend deux paramètres. \n# x est Un premier et y est celui qu'on ajoute\najoute <- function(x, y) {\n  x + y\n}\n\n# On peut maintenant utiliser cette foncction\najoute(3, 4)\n\n[1] 7\n\n# On peut effectuer les mêmes opérations. Les valeurs a et b sont encore \n# en mémoire, donc on peut faire :\najoute(a, b)\n\n[1] 7\n\nc <- ajoute(a, b)\nc\n\n[1] 7\n\najoute(c, a)\n\n[1] 10\n\n\nLes fonctions disposent d’une documentation qu’on peut explorer en utilisant l’aide.\n\nExercice pratique sur la recherche d ’aide.\n\n\nLe signe %>% est un “tuyau”. On peut le lire à haute voix comme “ensuite”. Par exemple :\n\n\nlibrary(tidyverse)\n\nd <- a %>%\n  ajoute(b) %>%\n  ajoute(c)\n\nd\n\n[1] 14\n\n\n\nna.rm : Les valeurs manquantes, notées NA dans R (certaines peuvent avoir pour valeur NaN). On utilise na.rm pour les éluder dans les opérations simples.\n\n\n# On commence par créer les variables (les colonnes du tableau)\nnoms <- c(\"John\", \"Jack\", \"Cindy\", \"Samantha\")\nsexe <- c(\"homme\", \"homme\", \"femme\", \"femme\")\nages <- c(42, 57, 24, NA)\npoids <- c(87, 73, NA, NA)\ntailles <- c(174, 198, 192, 164)\n\n# On les rassemble dans un tableau \nma_table <- data.frame(noms, sexe, ages, poids, tailles)\n\n# On peut faire une moyenne sur les tailles car on a toutes les variables\nmean(ma_table$tailles)\n\n[1] 182\n\nsum(ma_table$tailles)\n\n[1] 728\n\n# Mais la moyenne ne fonctionne pas immédiatement sur les poids ou les âges\n# car il manque des variables\nmean(ma_table$ages)\n\n[1] NA\n\nsum(ma_table$poids)\n\n[1] NA\n\n# Il faut préciser qu'il faut omettre les variables manquantes\nmean(ma_table$ages, na.rm = TRUE)\n\n[1] 41\n\nsum(ma_table$poids, na.rm = TRUE)\n\n[1] 160\n\n\n\nverbes :\n\nselect : choisir des colonnes\nfilter : choisir des lignes\nmutate : modifier des valeurs\ngroup_by : variables pour des tris\ncréer des filtres : summarise\n\n\n\n# Un exemple qui combine ces opérations\nma_table %>%\n  filter(!is.na(ages)) %>%\n  select(sexe, ages, tailles, poids) %>%\n  group_by(sexe) %>%\n  summarise(nb_pers = n(),\n            somme_poids = sum(poids, na.rm = TRUE),\n            taille_max = max(tailles, na.rm = TRUE),\n            age_moy = mean(ages, na.rm = TRUE))\n\n# A tibble: 2 × 5\n  sexe  nb_pers somme_poids taille_max age_moy\n  <chr>   <int>       <dbl>      <dbl>   <dbl>\n1 femme       1           0        192    24  \n2 homme       2         160        198    49.5\n\n\nDeux opérations particulière requièrent une étude plus approfondies\n\nJointures : fusionner deux tableaux par une variable d’identification (“clé”)\nPivots : passer un tableau de long en large\nmap : appliquer des opérations successives\nunnest : déplier des listes imbriquées\n\nUn point important est relatif aux types des variables : numérique, catégorielles, textes, dates, spatiales… En général, les opérations ne peuvent concerner que des variables du même type. Les fonctions sont souvent contraignantes quant aux types des variables qu’elles prennent comme arguments.\nPour une analyse plus approfondie, voir juba."
  },
  {
    "objectID": "10-fondamentaux_R.html#produire-des-cartes-simples-avec-r",
    "href": "10-fondamentaux_R.html#produire-des-cartes-simples-avec-r",
    "title": "11  Fondamentaux pour l’utilisation de R",
    "section": "11.4 Produire des cartes simples avec R",
    "text": "11.4 Produire des cartes simples avec R\n\n# Les librairies requises \nlibrary(sf) # pour traiter des données spatiales\nlibrary(tmap) # pour faire des cartes\n\n# Charger une carte des \ncarte <- st_read(\"data/Vahatra/Vahatra98AP.shp\") %>%\n  st_make_valid()\n\nReading layer `Vahatra98AP' from data source \n  `/home/florent/Documents/Stats/conservation-deforestation-madagascar2/data/Vahatra/Vahatra98AP.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 98 features and 12 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 43.25742 ymin: -25.60502 xmax: 50.47724 ymax: -11.98301\nCRS:           NA\n\n# On projette la carte\ntm_shape(carte) +\n  tm_polygons(col = \"cat__iucn\") +\n  tmap_options(check.and.fix = TRUE) + # Parce qu'on a quelques erreurs topo\n  tm_layout(legend.outside = TRUE)"
  },
  {
    "objectID": "10-fondamentaux_R.html#produire-des-graphiques-avec-r",
    "href": "10-fondamentaux_R.html#produire-des-graphiques-avec-r",
    "title": "11  Fondamentaux pour l’utilisation de R",
    "section": "11.5 Produire des graphiques avec R",
    "text": "11.5 Produire des graphiques avec R\nOn utilise le package ggplot, avec la syntaxe suivante.\n\n\nCode\n# On réalise un graphique simple\ncarte %>%\n  ggplot(aes(x = cat__iucn, y = hectares)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\nBarnier, Julien. 2022. “Introduction à r Et Au Tidyverse.” https://juba.github.io/tidyverse/index.html.\n\n\nGaliana, Lino, and Olivier Meslin, eds. 2022. “utilitR: Documentation Collaborative Sur r.” https://www.book.utilitr.org/.\n\n\nGörgen, Darius A., and Om Prakash Bhandari. 2022. Mapme.biodiversity: Efficient Monitoring of Global Biodiversity Portfolios. https://CRAN.R-project.org/package=mapme.biodiversity.\n\n\nGrolemund, Garrett, and Hadley Wickham. 2022. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 1st edition. Sebastopol, CA: O’Reilly Media. https://r4ds.had.co.nz/.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2022. Geocomputation with R. Boca Raton London New York: Routledge. https://geocompr.robinlovelace.net/."
  },
  {
    "objectID": "00-intro.html#cadrage-général-pour-les-méthodes-dévaluation",
    "href": "00-intro.html#cadrage-général-pour-les-méthodes-dévaluation",
    "title": "Introduction",
    "section": "Cadrage général pour les méthodes d’évaluation",
    "text": "Cadrage général pour les méthodes d’évaluation\nCliquer sur ce lien pour télécharger la présentation.\n\n\n\n\nBédécarrats, Florent, and Alexandre Hobeika. 2017. “Une Alternative à Word : Écrire En RMarkdown.” Billet. Data Sciences Sociales. http://data.hypotheses.org/1144.\n\n\nKluve, Jochen, Johannes Schielain, Melvin Wong, and Yota Eilers. 2022. “The KfW Protected Areas Portfolio: A Rigorous Impact Evaluation.” Frankfürt."
  },
  {
    "objectID": "04b-recap_donnees_dispo.html",
    "href": "04b-recap_donnees_dispo.html",
    "title": "5  Récapitulatif et description des données disponibles",
    "section": "",
    "text": "Cliquer sur ce lien pour la présentation générale des données mobilisées."
  },
  {
    "objectID": "08-diff_in_diff.html",
    "href": "08-diff_in_diff.html",
    "title": "10  Doubles différences",
    "section": "",
    "text": "Pour réaliser un diff-in-diff, on va utiliser la méthode de différence de différences échelonnées élaboréé par Callaway et Sant’Anna (2021b) et diffusée dans la librairie {did} (Callaway and Sant’Anna 2021a).\nLa spécification employée peut se traduire de la manière suivante :\nOn l’applique aux aires protégées et à leur déforestation entre 1990 et 2021. On commence par préparer le jeu de données tel qu’attendu par {did}, de sorte à obtenir :\n\n\nCode\n# On installe le package {did} si pas encore installé\nif (!(\"did\" %in% installed.packages())) {\n  install.packages(\"did\")\n}\nlibrary(did) # Pour des doubles-différences échelonnées\nlibrary(tidyverse) # Pur faciliter la manipulation de données\nlibrary(lubridate) # Pour modifier les dates\nlibrary(gt) # Pour de jolis tableaux\n\nload(\"data/ch3_AP_Vahatra.rds\") # On charge les données préparées au chapitre 3\noptions(scipen = 999) # On désactive les notations scientifiques\n\n# On prépare le jeu de données au format attendu par {did}\nap_did <- AP_Vahatra %>%\n  select(nom, date_creation, num_atlas = num_atlas_, starts_with(\"TMF\"),\n         cat_iucn = cat__iucn) %>%\n  mutate(annee_creation = year(date_creation)) %>%\n  pivot_longer(cols = starts_with(\"TMF\"), \n               names_to = \"TMF_variable\", \n               values_to = \"TMF_value\") %>%\n  mutate(annee = str_extract(TMF_variable, \"[:digit:]{4}\"),\n         annee = as.numeric(annee),\n         traitee = ifelse(annee > annee_creation, 1, 0),\n         TMF_variable = str_remove(TMF_variable, \"TMF\"),\n         TMF_variable = str_remove(TMF_variable, \"_[:digit:]{4}\"),\n         group = 1) %>%\n  pivot_wider(names_from = TMF_variable, values_from = TMF_value)\n\ngt(slice(ap_did, 20:30)) %>%\n  tab_header(title = \"Jeu de données 1990-2020 préparé pour {did}\",\n             subtitle = \"Echantillon des 10 lignes\") %>%\n  tab_source_note(\"Source : Aires protégées d'AP Vahatra, données TMF\")\n\n\n\n\n\n\n  \n    \n      Jeu de données 1990-2020 préparé pour {did}\n    \n    \n      Echantillon des 10 lignes\n    \n  \n  \n    \n      nom\n      date_creation\n      num_atlas\n      cat_iucn\n      annee_creation\n      annee\n      traitee\n      group\n      deg_HA\n      ly_HA\n      deg_ratio\n      ly_ratio\n    \n  \n  \n    Agnakatrika\n2015-04-28\n54\nVI\n2015\n2009\n0\n1\n3.51\n4.50\n0.008347603\n0.0107020548\n    Agnakatrika\n2015-04-28\n54\nVI\n2015\n2010\n0\n1\n5.04\n6.21\n0.013160987\n0.0162162162\n    Agnakatrika\n2015-04-28\n54\nVI\n2015\n2011\n0\n1\n1.62\n4.05\n0.004230317\n0.0105757932\n    Agnakatrika\n2015-04-28\n54\nVI\n2015\n2012\n0\n1\n14.58\n16.56\n0.038072855\n0.0432432432\n    Agnakatrika\n2015-04-28\n54\nVI\n2015\n2013\n0\n1\n18.36\n11.61\n0.047943596\n0.0303172738\n    Agnakatrika\n2015-04-28\n54\nVI\n2015\n2014\n0\n1\n0.54\n1.62\n0.001410106\n0.0042303173\n    Agnakatrika\n2015-04-28\n54\nVI\n2015\n2015\n0\n1\n3.33\n5.67\n0.008695652\n0.0148061105\n    Agnakatrika\n2015-04-28\n54\nVI\n2015\n2016\n1\n1\n6.30\n15.12\n0.016451234\n0.0394829612\n    Agnakatrika\n2015-04-28\n54\nVI\n2015\n2017\n1\n1\n21.24\n31.14\n0.055464160\n0.0813160987\n    Agnakatrika\n2015-04-28\n54\nVI\n2015\n2018\n1\n1\n2.16\n0.18\n0.005640423\n0.0004700353\n    Agnakatrika\n2015-04-28\n54\nVI\n2015\n2019\n1\n1\n2.34\n0.18\n0.006110458\n0.0004700353\n  \n  \n    \n      Source : Aires protégées d'AP Vahatra, données TMF\n    \n  \n  \n\n\n\n\nPour rappel, on récapitule les années d’assignation :\n\n\nCode\nap_did %>%\n  select(nom, annee_creation) %>%\n  filter(annee_creation > 1990) %>%\n  unique() %>%\n  group_by(annee_creation) %>%\n  summarize(`Nombre d'AP créées cette année` = n()) %>%\n  gt() %>%\n  tab_header(title = \"Années de création des aires protégées\")\n\n\n\n\n\n\n  \n    \n      Années de création des aires protégées\n    \n    \n  \n  \n    \n      annee_creation\n      Nombre d'AP créées cette année\n    \n  \n  \n    1991\n1\n    1997\n9\n    1998\n2\n    2002\n1\n    2003\n1\n    2004\n1\n    2007\n2\n    2011\n2\n    2012\n1\n    2015\n53"
  },
  {
    "objectID": "08-diff_in_diff.html#double-différences-pour-la-déforestation",
    "href": "08-diff_in_diff.html#double-différences-pour-la-déforestation",
    "title": "10  Doubles différences",
    "section": "10.2 Double différences pour la déforestation",
    "text": "10.2 Double différences pour la déforestation\nOn passe maintenant à l’estimation pour la déforestation.\n\n\nCode\nattgt_apmada_def <- att_gt(yname = \"ly_ratio\",\n                        tname = \"annee\",\n                        idname = \"num_atlas\",\n                        gname = \"annee_creation\",\n                        data = ap_did,\n                       control_group = \"notyettreated\")\nggdid(attgt_apmada_def) +\n  theme(axis.text.x =  element_text(angle = 45, hjust = 1))\n\n\n\n\n\nPeu d’effet ressortent graphiquement.\nOn aggrège les effets de traitment pour l’ensemble de la période\n\n\nCode\nagg.simple <- aggte(attgt_apmada_def, type = \"simple\", na.rm = TRUE)\n\nsummary(agg.simple)\n\n\n\nCall:\naggte(MP = attgt_apmada_def, type = \"simple\", na.rm = TRUE)\n\nReference: Callaway, Brantly and Pedro H.C. Sant'Anna.  \"Difference-in-Differences with Multiple Time Periods.\" Journal of Econometrics, Vol. 225, No. 2, pp. 200-230, 2021. <https://doi.org/10.1016/j.jeconom.2020.12.001>, <https://arxiv.org/abs/1803.09015> \n\n     ATT    Std. Error     [ 95%  Conf. Int.] \n -0.0001         0.001    -0.0021      0.0019 \n\n\n---\nSignif. codes: `*' confidence band does not cover 0\n\nControl Group:  Not Yet Treated,  Anticipation Periods:  0\nEstimation Method:  Doubly Robust\n\n\nCode\n# gt(summary(agg.simple)) %>%\n#   tab_header(title = \"Effet aggrégé pour les traités de 1991 à 2011\") %>%\n#   tab_footnote(\"Résultats encore préliminaire à confirmer (erreurs possibles)\")\n\n\nPour la déforestation, on ne voit aucun effet net des aires protégées (ATT très faible et non significativement différent de 0). Mais la taille de l’échantillon est extrêmement restreinte et il est on ne peut pas vraiment en tirer d’interprétation."
  },
  {
    "objectID": "08-diff_in_diff.html#double-différence-pour-la-dégradation",
    "href": "08-diff_in_diff.html#double-différence-pour-la-dégradation",
    "title": "10  Doubles différences",
    "section": "10.3 Double différence pour la dégradation",
    "text": "10.3 Double différence pour la dégradation\nOn modifie la spécification en remplaçant la variable à expliquer “déforestation” par “dégradation” (cf. présentation des sources dans Chapter 5).\n\n\nCode\nattgt_apmada_deg <- att_gt(yname = \"deg_ratio\",\n                        tname = \"annee\",\n                        idname = \"num_atlas\",\n                        gname = \"annee_creation\",\n                        data = ap_did,\n                       control_group = \"notyettreated\")\nggdid(attgt_apmada_deg) +\n  theme(axis.text.x =  element_text(angle = 45, hjust = 1))\n\n\n\n\n\nOn aggrège les résultats :\n\n\nCode\nagg.simple <- aggte(attgt_apmada_deg, type = \"simple\", na.rm = TRUE)\n\nsummary(agg.simple)\n\n\n\nCall:\naggte(MP = attgt_apmada_deg, type = \"simple\", na.rm = TRUE)\n\nReference: Callaway, Brantly and Pedro H.C. Sant'Anna.  \"Difference-in-Differences with Multiple Time Periods.\" Journal of Econometrics, Vol. 225, No. 2, pp. 200-230, 2021. <https://doi.org/10.1016/j.jeconom.2020.12.001>, <https://arxiv.org/abs/1803.09015> \n\n     ATT    Std. Error     [ 95%  Conf. Int.] \n -0.0052        0.0028    -0.0107      0.0003 \n\n\n---\nSignif. codes: `*' confidence band does not cover 0\n\nControl Group:  Not Yet Treated,  Anticipation Periods:  0\nEstimation Method:  Doubly Robust\n\n\nCode\n# gt(summary(agg.simple)) %>%\n#   tab_header(title = \"Effet aggrégé pour les traités de 1991 à 2011\") %>%\n#   tab_footnote(\"Résultats encore préliminaire à confirmer (erreurs possibles)\")\n\n\nLà encore, l’effet mesuré est très faible et non significativement différent de 0.\n\nMise en garde : l’échantillon utilisé à titre d’exemple ici est trop petit pour cette méthode. Il convient de préférer une analyse avec un plus grand nombre d’observation, par exemple en se focalisant sur des pixels ou des cellules.\n\n\n\n\n\nCallaway, Brantly, and Pedro H. C. Sant’Anna. 2021a. “Difference-in-Differences with Multiple Time Periods.” https://doi.org/10.1016/j.jeconom.2020.12.001.\n\n\n———. 2021b. “Difference-in-Differences with Multiple Time Periods.” Journal of Econometrics, Themed Issue: Treatment Effect 1, 225 (2): 200–230. https://doi.org/10.1016/j.jeconom.2020.12.001."
  }
]